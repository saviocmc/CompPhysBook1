
\chapter{Linear Algebra}\label{chap:linalgebra} 


\section{Introduction}

This chapter introduces several matrix related topics, from the solution of linear equations, computing determinants, conjugate-gradient methods, spline interpolation to efficient handling of matrices.
%%  Add more stuff about blas and lapack and armadillo
%%  add about svd as well
%%  Think of having least squares as well
%%  May consider splitting it into two or three chapters, one on matrix handling
%%  and usage of blas, armadillo etc
%%  One on linaer algebra problems, direct and iterative, add SVD
%%  One on minima and root searching, least squares and CG based methods

In this chapter
we  deal with basic matrix operations,
such as the solution of linear equations, calculate the inverse of
a matrix, its determinant etc. 
The solution of linear equations is an important part of numerical mathematics and arises
in many applications in the sciences. Here we focus in particular on so-called direct or elimination 
methods, which are
in principle determined through a finite number of arithmetic operations. Iterative methods will also be discussed.

This chapter serves also the purpose of 
introducing important programming details such as handling 
memory allocation for matrices and the usage of the libraries which follow these lectures.

The algorithms
we describe and their original source codes are taken from the widely used software
package LAPACK \cite{lapack}, which follows two other popular packages developed in the 1970s, 
namely EISPACK
and LINPACK. The latter was developed for linear equations 
and least square problems while the former 
was developed for solving symmetric, unsymmetric and generalized eigenvalue problems.
From LAPACK's website \url{http://www.netlib.org}  it is 
possible to download for free all source codes from 
this library. Both C++ and Fortran versions are available.  
Another important library is BLAS \cite{blas}, which stands for Basic Linear Algebra Subprogram.
It contains efficient codes for algebraic operations on vectors, matrices and vectors and matrices. 
Basically all modern supercomputer include this library, with efficient algorithms. 
Else, Matlab offers a very efficient
programming environment for dealing with matrices.
The classic text from where we have taken most of the formalism 
exposed here is the book on matrix computations
by Golub and Van Loan \cite{golub1996}. Good recent introductory texts are
Kincaid and Cheney \cite{kincaid} and Datta \cite{datta}. For more advanced ones see
Trefethen and Bau III
\cite{trefethen}, Kress \cite{kress}  and Demmel \cite{demmel}. Ref.~\cite{golub1996} contains an extensive
list of textbooks on eigenvalue problems and linear algebra. LAPACK \cite{lapack} contains also extensive 
listings to the research literature on matrix computations.
For the introduction  of  the auxiliary 
library Blitz++ \cite{blitzref}, which allows for a very efficient way of handling arrays in C++  
we refer to the online manual at 
\url{http://www.oonumerics.org}.  A library we highly recommend is Armadillo, see
\url{http://arma.sourceforge.org}. Armadillo is 
an open-source C++ linear algebra library 
aiming towards a good balance between speed and ease of use. Integer, floating point and complex numbers 
are supported, as well as a subset of trigonometric and statistics functions. 
Various matrix and vector operations are provided through optional integration with BLAS and LAPACK.

\section{Mathematical Intermezzo}

The matrices we will deal with are primarily square real symmetric or hermitian ones, assuming thereby that 
an $n\times n$ matrix ${\bf A}\in {\mathbb{R}}^{n\times n}$ for a real matrix\footnote{A reminder on 
mathematical symbols may be appropriate here. The symbol  ${\mathbb{R}}$ is the set of real numbers. Correspondingly, 
${\mathbb{N}}$, ${\mathbb{Z}}$ and ${\mathbb{C}}$ represent the set of natural, integer and complex
numbers, respectively. A symbol like ${\mathbb{R}}^{n}$ stands for an $n$-dimensional real Euclidean space, while 
$C[a,b]$ is the space of real or complex-valued continuous functions on the interval $[a,b]$, where the latter is a closed interval. Similalry,  $C^m[a,b]$ is the space of $m$-times continuously differentiable  functions on the interval $[a,b]$. For more symbols and notations, see the main text.}
and 
${\bf A}\in {\mathbb{C}}^{n\times n}$ for a complex matrix. 
For the sake of simplicity,  we take a matrix ${\bf A}\in {\mathbb{R}}^{4\times 4}$
 and a corresponding identity matrix ${\bf I}$
%
\begin{equation}
\label{eq-1}
 {\bf A} =
      \left( \begin{array}{cccc} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 a_{21} & a_{22} & a_{23} & a_{24} \\
                                   a_{31} & a_{32} & a_{33} & a_{34} \\
                                  a_{41} & a_{42} & a_{43} & a_{44} 
             \end{array} \right)
\hspace*{2cm} {\bf I} =
      \left( \begin{array}{cccc} 1 & 0 & 0 & 0 \\
                                 0 & 1 & 0 & 0 \\
                                 0 & 0 & 1 & 0 \\
                                 0 & 0 & 0 & 1 
             \end{array} \right),
\end{equation}
where $a_{ij}\in {\mathbb{R}}$. 
The inverse of a matrix, if it exists, is defined by 
% 
\[
{\bf A}^{-1} \cdot {\bf A} = I.
\]
In the following discussion, 
matrices are always two-dimensional arrays while 
vectors are one-dimensional arrays.   
In our nomenclature we will restrict boldfaced capitals letters
such as {\bf A} to represent a general matrix, which is a two-dimensional array, while
$a_{ij}$ refers to a matrix element with row number $i$ and column number $j$. Similarly, a vector
being a one-dimensional array, is labelled {\bf x} and represented as (for a real vector)
\[
%\label{eq-1}
 {\bf x}\in {\mathbb{R}}^n \iff
      \left( \begin{array}{c} x_{1}\\
                                 x_{2}\\
                                   x_{3}  \\
                                  x_{4} 
             \end{array} \right),
\]
with pertinent vector elements $x_{i}\in {\mathbb{R}}$. 
Note that this notation implies $x_{i}\in {\mathbb{R}}^{4\times 1}$ and that the members of ${\bf x}$ are
column vectors. The elements of  $x_{i}\in {\mathbb{R}}^{1\times 4}$ are row vectors.
%

Table~\ref{tab7-0} lists some essential features of various types of matrices one may encounter.
\begin{table}[htbp]
\label{tab7-0}
\caption{Matrix properties}
\begin{center}
\begin{tabular}{|l|l|l|}\hline
Relations               & Name       & matrix elements\\ \hline
$ {\bf A} = {\bf A}^{T}$ & symmetric & $a_{ij} = a_{ji} $ \\ 
$ {\bf A} = \left ({\bf A}^{T} \right )^{-1} $ & real orthogonal&
                   $\sum_k a_{ik} a_{jk} = \sum_k a_{ki} a_{kj} = \delta_{ij}$ \\
$ {\bf A} = {\bf A}^{*}  $ & real matrix& $a_{ij} = a_{ij}^{*}$\\
$ {\bf A} = {\bf A}^{\dagger}  $ &  hermitian& $a_{ij} = a_{ji}^{*}$\\
$ {\bf A} = \left ({\bf A}^{\dagger} \right )^{-1} $ & unitary& 
             $\sum_k a_{ik} a_{jk}^{*} = \sum_k a_{ki}^{*} a_{kj}
                                                  = \delta_{ij}$ \\ \hline
\end{tabular}
\end{center} 
\end{table}     
%
Some of the matrices we will encounter are listed here
\begin{enumerate}
\item Diagonal if $a_{ij}=0$ for $i\ne j$, 
\item Upper triangular if $a_{ij}=0$ for $i >j$, which for a $4\times 4$ matrix is of the form 
\[
  \left( \begin{array}{cccc} a_{11} & a_{12}& a_{13} & a_{14} \\
                                 0 & a_{22} & a_{23} & a_{24} \\
                                  0 & 0 & a_{33} & a_{34}\\
                                  0 & 0 & 0 & a_{nn} 
             \end{array} \right)
\]
\item Lower triangular if $a_{ij}=0$ for $i <j$
\[
\left( \begin{array}{cccc} a_{11} & 0 & 0 & 0 \\
                                 a_{21} & a_{22} & 0 & 0 \\
                                   a_{31} & a_{32} & a_{33} & 0 \\
                                  a_{41} & a_{42} & a_{43} & a_{44} 
             \end{array} \right)
\]
\item Upper Hessenberg if $a_{ij}=0$ for $i >j+1$, which is similar to a
upper triangular except that it has non-zero elements for the first subdiagonal row
\[
\left( \begin{array}{cccc} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 a_{21} & a_{22} & a_{23} & a_{24} \\
                                   0 & a_{32} & a_{33} & a_{34} \\
                                  0 & 0 & a_{43} & a_{44} 
             \end{array} \right)
\]
\item Lower Hessenberg if $a_{ij}=0$ for $i <j+1$
\[
\left( \begin{array}{cccc} a_{11} & a_{12} & 0 & 0 \\
                                 a_{21} & a_{22} & a_{23} & 0 \\
                                   a_{31} & a_{32} & a_{33} & a_{34} \\
                                  a_{41} & a_{42} & a_{43} & a_{44} 
             \end{array} \right)
\]
\item Tridiagonal if $a_{ij}=0$ for $|i -j|>1$
\[
\left( \begin{array}{cccc} a_{11} & a_{12} & 0 & 0 \\
                                 a_{21} & a_{22} & a_{23} & 0 \\
                                   0 & a_{32} & a_{33} & a_{34} \\
                                  0 & 0 & a_{43} & a_{44} 
             \end{array} \right)
\]
\end{enumerate}
There are many more examples, 
such as lower banded with bandwidth $p$ for $a_{ij}=0$ for $i > j+p$,
upper banded with bandwidth $p$ for $a_{ij}=0$ for $i < j+p$, 
block upper triangular, block lower triangular etc.

For a real  $n\times n$ matrix  ${\bf A}$ the following properties are all equivalent
\begin{enumerate}
\item If the inverse of   ${\bf A}$ exists, ${\bf A}$ is nonsingular.
\item The equation ${\bf Ax}=0$ implies ${\bf x}=0$.
\item The rows of ${\bf A}$ form a basis of ${\mathbb{R}}^n$.
\item  The columns of ${\bf A}$ form a basis of ${\mathbb{R}}^n$.
\item ${\bf A}$ is a product of elementary matrices.
\item $0$ is not an eigenvalue of ${\bf A}$.
\end{enumerate}

The basic matrix operations that we will deal with are addition and subtraction
\begin{equation}
{\bf A}= {\bf B}\pm{\bf C}  \Longrightarrow a_{ij} = b_{ij}\pm c_{ij},
\label{eq:mtxadd}
\end{equation}
scalar-matrix multiplication
\[
{\bf A}= \gamma{\bf B}  \Longrightarrow a_{ij} = \gamma b_{ij},
\]
vector-matrix multiplication 
\begin{equation}
{\bf y}={\bf Ax}   \Longrightarrow y_{i} = \sum_{j=1}^{n} a_{ij}x_j,
\label{eq:vecmtx}
\end{equation}
matrix-matrix multiplication 
\begin{equation}
{\bf A}={\bf BC}   \Longrightarrow a_{ij} = \sum_{k=1}^{n} b_{ik}c_{kj},
\label{eq:mtxmtx}
\end{equation}
transposition
\[
{\bf A}={\bf B}^T   \Longrightarrow a_{ij} = b_{ji},
\]
and if ${\bf A}\in {\mathbb{C}}^{n\times n}$, conjugation results in
\[
{\bf A}=\overline{{\bf B}}^T   \Longrightarrow a_{ij} = \overline{b}_{ji},
\]
where a variable $\overline{z}=x-\imath y$ denotes the complex conjugate 
of $z=x+\imath y$. 
In a similar way we have the following basic vector operations, namely
addition and subtraction
\[
{\bf x}= {\bf y}\pm{\bf z}  \Longrightarrow x_{i} = y_{i}\pm z_{i},
\]
scalar-vector multiplication
\[
{\bf x}= \gamma{\bf y}  \Longrightarrow x_{i} = \gamma y_{i},
\]
vector-vector multiplication (called Hadamard multiplication)
\[
{\bf x}={\bf yz}   \Longrightarrow x_{i} = y_{i}z_i,
\]
the inner or so-called dot product
\begin{equation}
c={\bf y}^T{\bf z}   \Longrightarrow c = \sum_{j=1}^{n} y_{j}z_{j},
\label{eq:innerprod}
\end{equation}
with $c$ a constant
and the outer product, which yields a matrix,
\begin{equation}
{\bf A}=  {\bf yz}^T \Longrightarrow  a_{ij} = y_{i}z_{j},
\label{eq:outerprod}
\end{equation}
Other important operations are vector and matrix norms.
A class of vector norms are the so-called $p$-norms 
\[
||{\bf x}||_p = (|x_1|^p+|x_2|^p+\dots + |x_n|^p)^{\frac{1}{p}},
\] 
where $p \ge 1$. 
The most important are the
1, 2 and $\infty$ norms given by
\[
 ||{\bf x}||_1 = |x_1|+|x_2|+\dots + |x_n|,
\]
\[
||{\bf x}||_2 = (|x_1|^2+|x_2|^2+\dots + |x_n|^2)^{\frac{1}{2}}=({\bf x}^T{\bf x})^{\frac{1}{2}},
\]
and 
\[
||{\bf x}||_{\infty} = \mathrm{max}\hspace{0.1cm} |x_i|,
\]
for $1\le i \le n$. 
From these definitions, one can derive several  important relations, of which the so-called Cauchy-Schwartz
inequality is of great importance for many algorithms. For any ${\bf x}$ and ${\bf y}$ being 
real-valued or complex-valued quantities, the  inner product space satisfies
\[
   |{\bf x}^T{\bf y}| \le ||{\bf x}||_2||{\bf y}||_2,
\]
and the equality is obeyed only if ${\bf x}$ and ${\bf y}$ are linearly dependent. An important relation which follows from
the Cauchy-Schwartz relation is the famous triangle relation, which states that for any ${\bf x}$ and ${\bf y}$ in 
a real or complex, the  inner product space satisfies
\[
   ||{\bf x}+{\bf y}||_2 \le ||{\bf x}||_2+||{\bf y}||_2.
\]
Proofs can be found in for example Ref.~\cite{golub1996}.
As discussed in chapter \ref{chap:numanalysis}, the analysis of the relative error is important in our studies
of loss of numerical precision. Using a vector norm we can define the relative error for the machine
representation of a vector ${\bf x}$. We assume that $fl({\bf x})\in {\mathbb{R}}^{n}$ 
is the machine representation of a vector ${\bf x}\in {\mathbb{R}}^{n}$. If ${\bf x}\ne 0$, we define 
the relative error as
\[
    \epsilon = \frac{||fl({\bf x})-{\bf x}||}{||{\bf x}||}.
\]
Using the $\infty$-norm one can define a relative error that can be translated into a statement on
the correct significant digits of $fl({\bf x})$,
\[
    \frac{||fl({\bf x})-{\bf x}||_ {\infty}}{||{\bf x}||_{\infty}}\approx 10^{-l},
\]
where the largest component of  $fl({\bf x})$ has roughly $l$ correct significant digits.

We can define similar matrix norms as well. The most frequently used are the Frobenius norm
\[
||{\bf A}||_F = \sqrt{\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2},
\]
and the $p$-norms
\[
||{\bf A}||_p = \frac{||{\bf A}{\bf x}||_p}{||{\bf x}||_p},
\]
assuming that ${\bf x} \ne 0$. We refer the reader to the text of Golub and Van Loan \cite{golub1996} for a further
discussion of these norms.

The way we implement these operations will be discussed below, as it depends on the programming language 
we opt for. 

  
\section{Programming Details}\label{sec:matrixdetails}

Many programming problems arise from improper treatment of
arrays. In this section we will discuss some important points such as
array declaration, memory allocation and array transfer between
functions. We distinguish between two cases: (a) array declarations
where the array size is given at compilation time, and (b) where the
array size is determined during the execution of the program, so-called
dymanic memory allocation. Useful references on C++ programming details, in particular on the use of
pointers and memory allocation, are Reek's text \cite{reek} on pointers in C, 
Berryhill's monograph \cite{berryhill} on scientific programming in C++ and finally Franek's text \cite{franek}
on memory as a programming concept in C and C++. Good allround texts on C++ programming in engineering and
science are the books by Flowers \cite{flowers} and Barton and Nackman \cite{barton}.
See also the online lecture notes on C++ at \url{http://heim.ifi.uio.no/~hpl/INF-VERK4830}. 
For Fortran  we recommend the online lectures at \url{http://folk.uio.no/gunnarw/INF-VERK4820}.  
These web pages contain extensive references to other C++ and Fortran  resources. Both web pages
contain  enough material, lecture notes and exercises, in order to serve as material for own studies. 
\begin{center}
\begin{figure}[hbtp]
\includegraphics[scale=0.8]{figures/Nebbdyr1.ps}
\caption{Segmentation fault, again and again! Alas, this is a situation you will most likely end up in,
unless you initialize, access, allocate and deallocate properly your arrays. Many program development environments
such as Dev C++ at \url{www.bloodshed.net} provide debugging possibilities. 
Beware however that there may be segmentation errors which occur due to errors in libraries of the
operating system. (Drawing: courtesy by Victoria Popsueva 2003.)}
\end{figure}
\end{center}
\subsection{Declaration of fixed-sized vectors and matrices}
%
In the program below we  discuss some 
essential features of vector and matrix handling where the dimensions are
declared  in the program code. 

In {\bf line a} we have a standard C++ declaration of a
vector. The compiler reserves memory to store
five integers. The elements are \verb?vec[0], vec[1],....,vec[4]?.
Note that the numbering of elements starts with zero.
Declarations of other data types are similar, including
structure data.

The symbol vec is an element in memory containing the address to the
first element \verb?vec[0]? and is a pointer to a vector of five integer elements.

In {\bf line b} we have a standard fixed-size C++ declaration of a
matrix. Again the elements start with zero, 
\verb?matr[0][0], matr[0][1], ....., matr[0][4], matr[1][0],....?
This sequence of elements also shows how data are stored in
memory. For example, the element \verb?matr[1][0]? follows \verb?matr[0][4]?.
This is important in order to produce an efficient code and avoid memory stride.

There is one further important point concerning matrix declaration. In a
similar way as for the symbol {\bf vec},  {\bf matr} is an element in memory
which contains an address to a vector of three elements, but now
these elements are not integers. Each element is a  vector of five
integers. This is the correct way to understand the
declaration in {\bf line b}. With respect to pointers this means that
matr is {\sl pointer-to-a-pointer-to-an-integer} which we can write 
$**$matr. Furthermore $*$matr is {\sl a-pointer-to-a-pointer} of five integers.
This interpretation is important when we want to transfer vectors and
matrices to a function.

In {\bf line c} we transfer \verb?vec[]? and \verb?matr[][]? to the function
sub\_1(). To be specific, we transfer the addresses of \verb?vec[]? and
matr[][] to sub\_1().
%

In {\bf line d} we have the function definition of subfunction(). The
{\bf int} vec[] is a pointer to an integer. Alternatively we could
write {\bf int} $*$vec. The first version is better. It shows that it is a
vector of several integers, but not how many. The second
version could equally well be used to transfer the address to a single
integer element. Such a declaration does not distinguish
between the two cases.

The next definition is {\bf int} \verb?matr[][5]?. This is
a pointer to a vector of five elements  and the compiler must
be told that each vector element contains five integers.
Here an alternative version could be int ($*$matr)[5] which
clearly specifies that matr is a pointer to a vector of five
integers.
\begin{lstlisting}
 int main()
{
   int k,m, row = 3, col = 5;
   int    vec[5];      // line a
   int    matr[3][5];  // line b
   //  Fill in vector vec
   for (k = 0; k < col; k++) vec[k] = k;
   // fill in matr
   for (m = 0; m < row; m++){              
       for (k = 0; k < col ; k++)  matr[m][k] = m + 10*k;
   }
   //  write out the vector
   cout << `` Content of vector vec:'' << endl;
   for (k = 0; k < col; k++){
       cout << vec[k] << endl;
   }
   //  Then write out the matrix
   cout << `` Content of matrix matr:'' << endl;    
   for (m = 0; m < row; m++){              
       for (k = 0; k < col ; k++){
          cout <<  matr[m][k] << endl;
       }
   }
   subfunction(row, col, vec, matr);      // line c
   return 0;
}  // end main function

void subfunction(int row, int col, int vec[], int matr[][5]);      // line d
{
   int k, m;
   //  write out the vector
   cout << `` Content of vector vec in subfunction:'' << endl;
   for (k = 0; k < col; k++){
       cout << vec[k] << endl;
   }
   //  Then write out the matrix
   cout << `` Content of matrix matr in subfunction:'' << endl;    
   for (m = 0; m < row; m++){              
       for (k = 0; k < col ; k++){
          cout <<  matr[m][k] << endl;
       }
   }
}  // end of function subfunction
\end{lstlisting}
There is at least one drawback with such a matrix declaration. If we want to
change the dimension of the matrix and replace 5 by something else we
have to do the same change in all functions where this matrix
occurs.

There is another point to note regarding
the declaration of variables in a function which 
includes vectors and matrices. When the execution of a function
terminates, the memory required for the variables is released. In the
present case memory for all variables in main() are reserved during
the whole program execution, but variables which are declared in
subfunction() are released when the execution returns to main().

\subsection{Runtime Declarations of Vectors and Matrices in C++}
We change thereafter our program in order to include dynamic allocation of arrays.
As mentioned in the previous subsection a fixed size declaration of
vectors and matrices before compilation is in many cases bad. You may
not know beforehand the actually needed sizes of vectors 
and matrices. In large projects where memory is a limited factor it could be
important to reduce memory requirement for matrices which are not used
any more. In C an C++ it is possible and common to postpone
size declarations of arrays untill you really know what you need and
also release memory reservations when it is not needed any more.
The following program shows how we could change the previous one with static declarations to dynamic allocation of arrays.
\begin{lstlisting}
 int main()
{
   int k,m, row = 3, col = 5;
   int    vec[5];      // line a
   int    matr[3][5];  // line b

   cout << `` Read in number of rows'' << endl;    // line c
   cin >> row;
   cout << `` Read in number of columns'' << endl;
   cin >> col;
   
   vec = new int[col];                            // line d
   matr = (int **)matrix(row,col,sizeof(int));    // line e
   //  Fill in vector vec
   for (k = 0; k < col; k++) vec[k] = k;
   // fill in matr
   for (m = 0; m < row; m++){              
       for (k = 0; k < col ; k++)  matr[m][k] = m + 10*k;
   }
   //  write out the vector
   cout << `` Content of vector vec:'' << endl;
   for (k = 0; k < col; k++){
       cout << vec[k] << endl;
   }
   //  Then write out the matrix
   cout << `` Content of matrix matr:'' << endl;    
   for (m = 0; m < row; m++){              
       for (k = 0; k < col ; k++){
          cout <<  matr[m][k] << endl;
       }
   }
   subfunction(row, col, vec, matr);      // line f
   free_matrix((void **) matr);           // line g
   delete vec[];
   return 0;
}  // end main function

void subfunction(int row, int col, int vec[], int matr[][5]);      // line h
{
   int k, m;
   //  write out the vector
   cout << `` Content of vector vec in subfunction:'' << endl;
   for (k = 0; k < col; k++){
       cout << vec[k] << endl;
   }
   //  Then write out the matrix
   cout << `` Content of matrix matr in subfunction:'' << endl;    
   for (m = 0; m < row; m++){              
       for (k = 0; k < col ; k++){
          cout <<  matr[m][k] << endl;
       }
   }
}  // end of function subfunction
\end{lstlisting}
In {\bf line a} we declare a pointer to an integer which later will be used to
store an address to the first element of a vector. Similarily, {\bf
line b} declares a pointer-to-a-pointer which will contain the address to a
pointer of row vectors, each with col integers. This will then become a
matrix with dimensionality [col][col]

In {\bf line c} we read in the size of vec[] and matr[][] through the numbers
row and col.

Next we reserve memory for the vector in {\bf line d}. 
In {\bf line e} we use a user-defined function to reserve necessary
memory for matrix[row][col] and again matr contains the address to the
reserved memory location.

The remaining part of the function main() are as in the previous case
down to {\bf line f}. Here we have a call to a user-defined function
which releases the reserved memory of the matrix. In this case this is
not done automatically.

In {\bf line g} the same procedure is performed for vec[]. In this
case the standard C++ library has the necessary function.

Next, in {\bf line h} an important difference from the previous case
occurs. First, the vector declaration is the same, but the matr
declaration is quite different. The corresponding parameter in the call
to sub\_1[] in {\bf line g} is
 a double pointer. Consequently, matr in
{\bf line h} must be a double pointer.
 
Except for this difference sub\_1() is the same as before. 
The new feature in the program below is the call to the
user-defined functions {\bf matrix} and {\bf free\_matrix}. These functions
are defined in the library file {\bf lib.cpp}. The code for 
the dynamic memory allocation 
is given below.  
\lstset{language=c++}
\begin{lstlisting}[title={\url{http://folk.uio.no/compphys/programs/FYS3150/cpp/cplusplus library/lib.cpp}}]
  /*
   * The function                             
   *      void  **matrix()                    
   * reserves dynamic memory for a two-dimensional matrix 
   * using the C++ command new . No initialization of the elements. 
   * Input data:                      
   *  int row      - number of  rows          
   *  int col      - number of columns        
   *  int num_bytes- number of bytes for each 
   *                 element                  
   * Returns a void  **pointer to the reserved memory location.   
   */

void **matrix(int row, int col, int num_bytes)
  {
  int      i, num;
  char     **pointer, *ptr;

  pointer = new(nothrow) char* [row];
  if(!pointer) {
    cout << "Exception handling: Memory allocation failed";
    cout << " for "<< row << "row addresses !" << endl;
    return NULL;
  }
  i = (row * col * num_bytes)/sizeof(char);
  pointer[0] = new(nothrow) char [i];
  if(!pointer[0]) {
    cout << "Exception handling: Memory allocation failed";
    cout << " for address to " << i << " characters !" << endl;
    return NULL;
  }
  ptr = pointer[0];
  num = col * num_bytes;
  for(i = 0; i < row; i++, ptr += num )   {
    pointer[i] = ptr; 
  }
  return  (void **)pointer;
  } // end: function void **matrix()
\end{lstlisting}

As an alternative, you could write your own allocation and deallocation of matrices.
This can be done rather straightforwardly with the following statements.
Recall first that  a  matrix is represented by a double pointer
 that points to a contiguous memory segment holding a
sequence of double* pointers in case our matrix is a double precision variable. Then each double* 
pointer points to a row in the matrix. A declaration like 
\verb?double** A;? means that 
A$[i]$ is a pointer to the $i+1$-th row A$[i]$ and   A$[i][j]$ is matrix entry $(i,j)$.
The way we would allocate memory for such a matrix of dimensionality $n\times n$ is for example using the following piece of code
\lstset{language=c++}
\begin{lstlisting}
int n;
double **  A;

A = new double*[n]
for ( i = 0; i < n; i++)
    A[i] = new double[N];
\end{lstlisting}
When we declare a matrix (a two-dimensional array) we must first declare an array of double variables. To each of this
variables we assign an allocation of a single-dimensional array.
A conceptual picture on how a matrix ${\bf A}$ is stored in memory is shown in Fig.~\ref{fig:memoryalloc}.

\begin{figure}
\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(120,100)
\put(0,70){\large $\mathrm{double **A}$}
\put(40,70){\large $\Longrightarrow\mathrm{double *A[0\dots 3]}$}
\put(72,53){\large $\mathrm{A[0][0]}$}
\put(87,53){\large $\mathrm{A[0][1]}$}
\put(102,53){\large $\mathrm{A[0][2]}$}
\put(117,53){\large $\mathrm{A[0][3]}$}
\put(72,38){\large $\mathrm{A[1][0]}$}
\put(87,38){\large $\mathrm{A[1][1]}$}
\put(102,38){\large $\mathrm{A[1][2]}$}
\put(117,38){\large $\mathrm{A[1][3]}$}
\put(72,23){\large $\mathrm{A[2][0]}$}
\put(87,23){\large $\mathrm{A[2][1]}$}
\put(102,23){\large $\mathrm{A[2][2]}$}
\put(117,23){\large $\mathrm{A[2][3]}$}
\put(72,8){\large $\mathrm{A[3][0]}$}
\put(87,8){\large $\mathrm{A[3][1]}$}
\put(102,8){\large $\mathrm{A[3][2]}$}
\put(117,8){\large $\mathrm{A[3][3]}$}
\put(70,0){\grid(60,60)(15,15)}
\put(10,0){\grid(15,60)(15,15)}
\put(12,53){\large $\mathrm{A[0]}$}
\put(12,38){\large $\mathrm{A[1]}$}
\put(12,23){\large $\mathrm{A[2]}$}
\put(12,8){\large $\mathrm{A[3]}$}
\end{picture}
\end{center}
\caption{Conceptual representation of the allocation of a matrix in C++. \label{fig:memoryalloc}}
\end{figure}
Allocated memory should always be deleted when it is no longer needed.
We free memory using the statements 
\lstset{language=c++}
\begin{lstlisting}
for ( i = 0; i < n; i++)
    delete[] A[i];
delete[] A;
\end{lstlisting}
\lstinline{delete [] A;}, which frees an array of pointers to matrix rows.



However, including a library like Blitz++ \url{http://www.oonumerics.org}  or Armadillo 
makes life much easier
when dealing with matrices. 


\subsection{Matrix Operations and C++ and Fortran  Features of Matrix handling}

Many program libraries for scientific computing are written
in Fortran, often also in older version such as Fortran 77. 
When using functions from such program libraries, there are some
differences between C++ and Fortran  encoding of matrices
and vectors worth noticing.
Here are some simple guidelines in order to avoid
some of the most common pitfalls.

First of all, when we think of 
an $n\times n$ matrix in Fortran and C++, we typically would 
have a mental picture of a two-dimensional block of
stored numbers. The computer stores them however as sequential
strings of numbers. The latter could be stored as row-major order
or column-major order.
What do we mean by that? Recalling that for our 
matrix elements $  a_{ij}$, $i$
refers to rows and $j$ to columns, we could store a matrix 
in the sequence
$a_{11}a_{12}\dots a_{1n}a_{21}a_{22}\dots a_{2n}\dots a_{nn}$
if it is row-major order (we go along a given row $i$ and pick up
all column elements $j$) or it could be stored in column-major 
order 
$a_{11}a_{21}\dots a_{n1}a_{12}a_{22}\dots a_{n2}\dots a_{nn}$.

Fortran stores matrices in the latter way, i.e., by column-major,
while C++ stores them by row-major. 
It is crucial to keep this in mind when we are dealing 
with matrices, because if we were to organize the matrix
elements in the wrong way, important properties like
the transpose of a real matrix or the inverse can
be wrong, and obviously yield wrong physics.
Fortran subscripts begin typically with $1$, although
it is no problem in starting with zero, while C++ starts
with $0$ for the first element. This means that
$A(1,1)$ in Fortran is equivalent to $A[0][0]$ in C++.
Moreover, since the sequential storage in memory 
means that nearby matrix elements are close to each 
other in the memory locations (and thereby easier
to fetch) , operations involving e.g., additions of matrices
may take more time if we do not respect the given ordering. 

To see this, consider the following coding of matrix addition
in C++ and Fortran.
We have  $n\times n$ matrices ${\bf A}$, ${\bf B}$ and ${\bf C}$ and we wish to 
evaluate ${\bf A}={\bf B+C}$ according to Eq.~(\ref{eq:mtxadd}). In C++ this would be coded like
\lstset{language=c++}  
\begin{lstlisting}
   for(i=0 ; i < n ; i++) {  
      for(j=0 ; j < n ; j++) {
         a[i][j]=b[i][j]+c[i][j]
      }
   }  
\end{lstlisting}
while in Fortran we would have 

\lstset{language=[90]Fortran} 
\begin{lstlisting}
   DO  j=1,  n
      DO i=1, n
         a(i,j)=b(i,j)+c(i,j)
      ENDDO
   ENDDO
\end{lstlisting}
Fig.~\ref{fig:ccwaymatrix} shows how a $3\times 3$ matrix ${\bf A}$ is stored in both row-major and column-major
ways.
\begin{figure}
\begin{center}
\setlength{\unitlength}{1mm}
\begin{picture}(135,140)
%
\put(47,128){\large $a_{11}$}
\put(62,128){\large $a_{12}$}
\put(77,128){\large $a_{13}$}
\put(47,113){\large $a_{21}$}
\put(62,113){\large $a_{22}$}
\put(77,113){\large $a_{23}$}
\put(47,98){\large $a_{31}$}
\put(62,98){\large $a_{32}$}
\put(77,98){\large $a_{33}$}
\put(45,90){\grid(45,45)(15,15)}
%
\put(100,105){$\Longrightarrow$}
\put(35,105){$\Longleftarrow$}
%
\put(0,0){\grid(15,135)(15,15)}
\put(2,128){\large $a_{11}$}
\put(2,113){\large $a_{12}$}
\put(2,98){\large $a_{13}$}
\put(2,83){\large $a_{21}$}
\put(2,68){\large $a_{22}$}
\put(2,53){\large $a_{23}$}
\put(2,38){\large $a_{31}$}
\put(2,23){\large $a_{32}$}
\put(2,8){\large $a_{33}$}
%
\put(120,0){\grid(15,135)(15,15)}
\put(122,128){\large $a_{11}$}
\put(122,113){\large $a_{21}$}
\put(122,98){\large $a_{31}$}
\put(122,83){\large $a_{12}$}
\put(122,68){\large $a_{22}$}
\put(122,53){\large $a_{32}$}
\put(122,38){\large $a_{13}$}
\put(122,23){\large $a_{23}$}
\put(122,8){\large $a_{33}$}
\end{picture}
\end{center}
\caption{Row-major storage of a matrix to the left (C++ way) and column-major to the right (Fortran way). \label{fig:ccwaymatrix}}
\end{figure}

Interchanging the order of $i$ and $j$ can lead to a considerable
enhancement in process time. 
In Fortran  we write the above statements in a much simpler
way
\lstinline{a=b+c}.
However, the addition still involves $\sim n^2$ operations. 
Matrix multiplication or taking the 
inverse requires $\sim n^3$ operations. 
The matrix multiplication of Eq.~(\ref{eq:mtxmtx}) 
of two matrices ${\bf A}={\bf BC}$ could then take
the following form in C++
\lstset{language=c++} 
\begin{lstlisting}
   for(i=0 ; i < n ; i++) {  
      for(j=0 ; j < n ; j++) {
         for(k=0 ; k < n ; k++) {
            a[i][j]+=b[i][k]*c[k][j]
         }
      }
   }  
\end{lstlisting}
and in Fortran  we have
\lstset{language=[90]Fortran} 
\begin{lstlisting}
   DO  j=1,  n
      DO i=1, n
         DO k = 1, n
            a(i,j)=a(i,j)+b(i,k)*c(k,j)
         ENDDO
      ENDDO
   ENDDO
\end{lstlisting}
However, Fortran has an intrisic function called MATMUL, and
the above three loops can be coded in a single statement
\lstinline{a=MATMUL(b,c)}.
Fortran contains several array manipulation statements, such as
dot product of vectors, the transpose of a matrix 
etc etc. 
The outer product of two vectors is however not included in Fortran.
The coding of Eq.~(\ref{eq:outerprod}) takes then the following form in C++
\lstset{language=c++} 
\begin{lstlisting}
   for(i=0 ; i < n ; i++) {  
      for(j=0 ; j < n ; j++) {
          a[i][j]+=x[i]*y[j]
      }
   }  
\end{lstlisting}
and in Fortran we have
\lstset{language=[90]Fortran} 
\begin{lstlisting}
   DO  j=1,  n
      DO i=1, n
           a(i,j)=a(i,j)+x(j)*y(i)
      ENDDO
   ENDDO
\end{lstlisting}

A matrix-matrix multiplication of a general $n\times n$ matrix  with 
\[
            a(i,j)=a(i,j)+b(i,k)*c(k,j),
\]
in its inner loops requires
a multiplication and an addition. 
We define now a flop (floating point operation) as one of the following floating
point arithmetic operations, viz addition, subtraction, multiplication and division.
The above two floating point operations (flops)
are done $n^3$ times meaning that a general matrix multiplication requires
$2n^3$ flops if we have a square matrix. If we assume that our computer performs
$10^9$ flops per second, then to perform a matrix multiplication of 
a $1000\times 1000$ case should take two seconds.
This can be reduced if we multiply two matrices which are upper triangular such
as 
\[
%\label{eq-1}
 {\bf A} =
      \left( \begin{array}{cccc} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 0 & a_{22} & a_{23} & a_{24} \\
                                  0 & 0 & a_{33} & a_{34} \\
                                  0 & 0 & 0 & a_{44} 
             \end{array} \right).
\]
The multiplication of two upper triangular  matrices ${\bf BC}$ yields 
another upper triangular matrix ${\bf A}$, resulting in the following C++
code
\lstset{language=c++} 
\begin{lstlisting}
   for(i=0 ; i < n ; i++) {  
      for(j=i ; j < n ; j++) {
         for(k=i ; k < j ; k++) {
            a[i][j]+=b[i][k]*c[k][j]
         }
      }
   }  
\end{lstlisting}
The fact that we have the constraint $ i \le j$ leads to the requirement for the computation
of $a_{ij}$ of 
$2(j-i+1)$ flops. The total number of flops is then
\[
  \sum_{i=1}^{n} \sum_{j=1}^{n} 2(j-i+1)=  \sum_{i=1}^{n} \sum_{j=1}^{n-i+1} 2j \approx
\sum_{i=1}^{n} \frac{2(n-i+1)^2}{2},
\]
where we used  that $\sum_{j=1}^n j = n(n+1)/2\approx n^2/2$ for large $n$ values. 
Using in addition that $\sum_{j=1}^n j^2 \approx n^3/3$ for large $n$ values,
we end up with approximately $n^3/3$ flops for the multiplication of two upper triangular 
matrices. 
This means that if we deal with matrix multiplication of upper triangular matrices,
we reduce the number of flops by a factor six if we code our matrix multiplication
in an efficient way. 

It is also important to keep in mind that computers are finite,
we can thus not store infinitely large matrices.
To calculate the space needed in memory for an
$n\times n$ matrix with double precision, 64 bits or 8 bytes
for every matrix element, one needs simply compute
$n\times n \times 8$ bytes . Thus, if $n=10000$, we will need
close to 1GB of storage. Decreasing the precision to 
single precision, only halves our needs.

A further point we would like to stress, is that one should
in general avoid fixed (at compilation time) dimensions 
of matrices. That is, one could always specify that a
given matrix ${\bf A}$ should have size $A[100][100]$, while
in the actual execution one may use only $A[10][10]$.
If one has several such matrices, one may run out of 
memory, while the actual processing of the program
does not imply that. Thus, we will always recommend that you use
dynamic memory allocation, and deallocation of arrays
when they are no longer needed. In Fortran one uses
the intrisic functions {\bf ALLOCATE} and {\bf DEALLOCATE}, while C++ employs 
the functions {\bf new} and {\bf delete}. 


\subsubsection{Strassen's algorithm}\label{subsubsec:strassenalgo}
As we have seen, the straightforward algorithm for matrix-matrix multiplication will require
$p$ multiplications and $p-1$ additions for each of the $m\times n$
elements. The total number of floating-point operations is then
$mn(2p-1) \sim \mathcal{O}(mnp)$. When the matrices $A$ and $B$ can be
divided into four equally sized blocks,
\begin{equation}
\begin{bmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{bmatrix}
=
\begin{bmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22} 
\end{bmatrix}
\begin{bmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22}
\end{bmatrix} ,
\end{equation}
we get eight multiplications of smaller blocks,
\begin{equation}
\begin{bmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{bmatrix}
=
\begin{bmatrix}
A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22} \\
A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12} + A_{22} B_{22} 
\end{bmatrix}
.
\end{equation}


Strassen discovered in 1968 how the number of multiplications could be reduced from eight to seven~\cite{golub1996}.
Following Strassen's approach we define some intermediates,
\begin{equation}
\label{eq:OpenCL:Strassen_intermediates}
\begin{matrix}
S_1 = A_{21} + A_{22}, & T_1 = B_{12} - B_{11},\\
S_2 = S_1 - A_{11}, & T_2 = B_{22} - T_1,\\
S_3 = A_{11} - A_{21}, & T_3 = B_{22} - B_{12},\\
S_4 = A_{12} - S_2, & T_4 = B_{21} - T_2 ,
\end{matrix}
\end{equation}
and need seven multiplications,
\begin{equation}
\label{eq:OpenCL:Strassen_multiplications}
\begin{matrix}
P_1 &= A_{11} B_{11}, & U_1 = P_1 + P_2, \\
P_2 &= A_{12} B_{21}, & U_2 = P_1 + P_4, \\
P_3 &= S_1 T_1, & U_3 = U_2 + P_5,\\
P_4 &= S_2 T_2, & U_4 = U_3 + P_7,\\
P_5 &= S_3 T_3, & U_5 = U_3 + P_3,\\
P_6 &= S_4 B_{22}, & U_6 = U_2 + P_3,\\
P_7 &= A_{22} T_4, & U_7 = U_6 + P_6 ,
\end{matrix} 
\end{equation}
to find the resulting $C$ matrix as
\begin{equation}
\begin{bmatrix}
C_{11} & C_{12} \\
C_{21} & C_{22}
\end{bmatrix}
=
\begin{bmatrix}
U_1 & U_7 \\
U_4 & U_5 
\end{bmatrix}
.
\end{equation}
In spite of the seemingly additional work, we have reduced the number
of multiplications from eight to seven. Since the multiplications are the
computational bottleneck compared to addition and subtraction, the
number of flops are reduced.

In the case of square $n\times n$ matrices with $n$ equal to a power of two, $n=2^m$, the divided blocks will have $\frac{n}{2} = 2^{m-1}$.
Letting $f(m)$ be the number of flops needed for the full matrix and applying Strassen recursively we find the total number of flops to be
\begin{equation}
f(m) = 7 f(m-1) = 7^2 f(m-2) = \cdots = 7^m f(0) , 
\end{equation}
where $f(0)$ is the one floating-point operation needed for multiplication of two numbers (two $2^0\times 2^0$ matrices).
For large matrices this can prove efficient, yielding a much better scaling, 
\begin{equation}
\mathcal{O}\left( 7^m \right) = 
\mathcal{O}\left( 2^{\log_2 7^m} \right) = 
\mathcal{O}\left( 2^{m \log_2 7} \right) = 
\mathcal{O}\left( n^{\log_2 7} \right) \approx
\mathcal{O}\left( n^{2.807} \right) ,
\end{equation}
effectively saving $7/8 = 12.5\%$ each time it is applied.




\subsubsection{Fortran Allocate Statement and Mathematical Operations on Arrays}
  An array is declared in the declaration section of a program, module, or procedure using
the dimension attribute. Examples include

\lstset{language=[90]Fortran}  
\begin{lstlisting}

     REAL, DIMENSION (10) :: x,y
     REAL, DIMENSION (1:10) :: x,y
     INTEGER, DIMENSION (-10:10) :: prob
     INTEGER, DIMENSION (10,10) :: spin 
\end{lstlisting}
  The default value of the lower bound of an array is 1. For this reason the first two
statements are equivalent to the first. 
The lower bound of an array can be negative. 
The last two statements are examples of two-dimensional arrays. 

  Rather than assigning each array element explicitly, we can use an array constructor to
give an array a set of values. An array constructor is a one-dimensional list of values,
separated by commas, and delimited by "(/" and "/)". An example is 

\lstset{language=[90]Fortran} 
\begin{lstlisting}

     a(1:3) = (/ 2.0, -3.0, -4.0 /)
\end{lstlisting}
is equivalent to the separate assignments 
%
\lstset{language=[90]Fortran} 
\begin{lstlisting}

     a(1) = 2.0
     a(2) = -3.0
     a(3) = -4.0
\end{lstlisting}

One of the better features of Fortran is dynamic storage allocation. That is, the size of
an array can be changed during the execution of the program. 
To see how the dynamic allocation works in Fortran, consider the
following simple example where we set up a $4\times 4 $ unity matrix.

\lstset{language=[90]Fortran} 
\begin{lstlisting} 
       ......
       IMPLICIT NONE
!      The definition of the matrix, using dynamic allocation
       REAL, ALLOCATABLE, DIMENSION(:,:) :: unity
!      The size of the matrix
       INTEGER :: n
!      Here we set the dim n=4
       n=4
!  Allocate now place in memory for the matrix
       ALLOCATE ( unity(n,n) )
!  all elements are set equal zero
       unity=0.
!      setup identity matrix
       DO i=1,n
          unity(i,i)=1.
       ENDDO
       DEALLOCATE ( unity)
       .......
\end{lstlisting}
We always recommend to use the deallocation statement, since this frees
space in memory. 
If the matrix is transferred to a function from a calling program,
one can transfer the dimensionality $n$ of that matrix with the call.
Another possibility is to determine the dimensionality with the
\verb$SIZE$ function. Writing a statement like 
\lstinline{n=SIZE(unity,DIM=1)}
gives the number of  rows, while  using DIM=2 gives the number of 
columns. Note however that this involves an extra call to a function. If 
speed matters, one should avoid such calls.



\section{Linear Systems}

In this section we outline some of the most used algorithms to solve sets of linear equations.
These algorithms are based on Gaussian elimination \cite{golub1996,kress} and will allow us to catch
several birds with a stone. We will show how to rewrite a matrix ${\bf A}$ in terms of an upper and a lower
triangular matrix, from which we easily can solve linear equation, compute the inverse of ${\bf A}$ and
obtain the determinant.  We start with Gaussian elimination, move to the more efficient 
LU-algorithm, which forms the basis for many linear algebra applications, and end the discussion 
with special cases such as the Cholesky decomposition and linear system of equations with a tridiagonal matrix.
 
We begin however with an example which demonstrates the importance
of being able to solve linear equations. 
Suppose we want to solve the following boundary value equation
\[
  -\frac{d^2u(x)}{dx^2} = f(x,u(x)),
\]
with $x\in (a,b)$ and with boundary conditions $u(a)=u(b) = 0$.
We assume that $f$ is a continuous function in the domain $x\in (a,b)$.
Since, except the few cases where it is possible to find analytic solutions, we
will seek approximate solutions, we choose to represent the approximation to the second derivative 
from the previous chapter 
\[
  f''=\frac{f_h -2f_0 +f_{-h}}{h^2} +O(h^2).
\]
We subdivide our interval $x\in (a,b)$ into $n$ subintervals by setting $x_i = a+ih$, with $i=0,1,\dots,n+1$.
The step size is then given by $h=(b-a)/(n+1)$ with $n\in {\mathbb{N}}$.
For the internal grid points $i=1,2,\dots n$ we replace the differential operator with the above formula
resulting in
\[
u''(x_i) \approx  \frac{u(x_i+h) -2u(x_i) +u(x_i-h)}{h^2},
\]
which we rewrite as 
\[
u^{''}_i \approx  \frac{u_{i+1} -2u_i +u_{i-i}}{h^2}.
\]
We can rewrite our original differential equation in terms of a discretized equation with approximations to the 
derivatives as
\[
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
\]
with $i=1,2,\dots, n$. We need to add to this system the two boundary conditions $u(a) =u_0$ and $u(b) = u_{n+1}$.
If we define a matrix 
\[
    {\bf A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
\]
and the corresponding vectors ${\bf u} = (u_1, u_2, \dots,u_n)^T$ and 
${\bf f}({\bf u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T$  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns 
\begin{equation} 
   {\bf A}{\bf u} = {\bf f}({\bf u}).
%   \label{eq:tridiageq}
\end{equation}
We assume that the solution $u$ exists and is unique for the exact differential equation, viz that the boundary
value problem has a solution. But
the discretization of the above differential equation leads to several questions, such as how well does the approximate solution
resemble the exact one as $h\rightarrow 0$, or does a given small value of $h$ allow us to establish existence and uniqueness of the solution. 

Here we specialize to two particular cases.
Assume first that the function $f$ does not depend on $u(x)$. Then our linear equation reduces to 
\begin{equation} 
   {\bf A}{\bf u} = {\bf f},
\label{eq:simpletriag}
\end{equation}
which is nothing but a simple linear equation with a tridiagonal matrix ${\bf A}$. We will solve such a system of equations
in subsection \ref{subsec:lineq}.

If we assume that our boundary value problem is that of a quantum mechanical particle confined by a harmonic
oscillator potential, then our function $f$ takes the form (assuming that all constants $m=\hbar=\omega=1$) 
$f(x_i,u(x_i))= -x_i^2u(x_i)+2\lambda u(x_i)$ with $\lambda$ being the eigenvalue. Inserting this into our equation,
we define first a new matrix ${\bf A}$ as 
\begin{equation}
    {\bf A}= \left(\begin{array}{cccccc}
                          \frac{2}{h^2}+x_1^2 & -\frac{1}{h^2} &  &   &  & \\
                          -\frac{1}{h^2} & \frac{2}{h^2}+x_2^2 & -\frac{1}{h^2} & & & \\
                           & -\frac{1}{h^2} & \frac{2}{h^2}+x_3^2 & -\frac{1}{h^2} & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-\frac{1}{h^2}  &\frac{2}{h^2}+x_{n-1}^2& -\frac{1}{h^2} \\
                           &    &  &   &-\frac{1}{h^2} & \frac{2}{h^2} +x_n^2\\
                      \end{array} \right),  
%\label{eq:simpletriag1}
\end{equation}
which  leads to the following eigenvalue problem
\[
\left(\begin{array}{cccccc}
                          \frac{2}{h^2}+x_1^2 & -\frac{1}{h^2} &  &   &  & \\
                          -\frac{1}{h^2} & \frac{2}{h^2}+x_2^2 & -\frac{1}{h^2} & & & \\
                           & -\frac{1}{h^2} & \frac{2}{h^2}+x_3^2 & -\frac{1}{h^2} & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-\frac{1}{h^2}  &\frac{2}{h^2}+x_{n-1}^2& -\frac{1}{h^2} \\
                           &    &  &   &-\frac{1}{h^2} & \frac{2}{h^2} +x_n^2\\
                      \end{array} \right)\left(\begin{array}{c}
                           u_1\\
                           u_2\\
                           \\
                           \\
                           \\
                           u_n\\
                      \end{array} \right)
  =
2\lambda\left(\begin{array}{c}
                           u_1\\
                           u_2\\
                           \\
                           \\
                           \\
                           u_n\\
                      \end{array} \right).
\]
We will solve this type of equations in chapter \ref{chap:eigenvalue}. These lecture notes contain however several other
examples of rewriting mathematical expressions into matrix problems. In chapter \ref{chap:integrate} we show how a 
set of linear integral equation when discretized can be transformed into a simple matrix inversion problem.
The specific example we study in that chapter is the rewriting of Schr\"odinger's equation for scattering problems.
Other examples of linear equations will appear in our discussion of ordinary and partial differential equations. 





\subsection{Gaussian Elimination}

Any discussion on the solution of linear equations should start with Gaussian elimination. This text is no exception.
We start with the linear set of equations
\[
   {\bf A}{\bf x} = {\bf w}.
\]
We assume also that the matrix ${\bf A}$ is non-singular and that the 
matrix elements along the diagonal satisfy $a_{ii} \ne 0$. We discuss later how to
handle such cases. 
In the discussion we limit ourselves again to a matrix ${\bf A}\in {\mathbb{R}}^{4\times 4}$, 
resulting in a set of linear equations of the form
%
\[
\left(\begin{array}{cccc}
                           a_{11}& a_{12} &a_{13}& a_{14}\\
                           a_{21}& a_{22} &a_{23}& a_{24}\\
                           a_{31}& a_{32} &a_{33}& a_{34}\\
                           a_{41}& a_{42} &a_{43}& a_{44}\\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           w_1\\
                           w_2\\
                           w_3 \\
                           w_4\\
                      \end{array} \right).
\]
or
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4. \nonumber
\end{eqnarray}
The basic idea of Gaussian elimination is to use the first equation to eliminate the first unknown $x_1$
from the remaining $n-1$ equations. Then we use the new second equation to eliminate the second unknown
$x_2$ from the remaining $n-2$ equations. With $n-1$ such eliminations
we obtain a so-called upper triangular set of equations of the form
\begin{eqnarray}\label{eq:gaussbacksub}
 b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
 b_{22}x_2 + b_{23}x_3 + b_{24}x_4=&y_2 \nonumber \\
b_{33}x_3 + b_{34}x_4=&y_3 \nonumber \\
b_{44}x_4=&y_4. \nonumber
\end{eqnarray}
We can solve this system of equations recursively starting from $x_n$ (in our case $x_4$) and proceed with 
what is called a backward substitution. This process can be expressed mathematically as
\[
   x_m = \frac{1}{b_{mm}}\left(y_m-\sum_{k=m+1}^nb_{mk}x_k\right)\hspace{0.5cm} m=n-1,n-2,\dots,1.
\]
To arrive at such an upper triangular system of equations, we start by eliminating
the unknown $x_1$ for $j=2,n$. We achieve this by multiplying the first equation by $a_{j1}/a_{11}$ and then subtract
the result from the $j$th equation. We assume obviously that $a_{11}\ne 0$ and that
${\bf A}$ is not singular. We will come back to this problem below.

Our actual $4\times 4$ example reads after the first operation
\[
\left(\begin{array}{cccc}
                           a_{11}& a_{12} &a_{13}& a_{14}\\
                           0& (a_{22}-\frac{a_{21}a_{12}}{a_{11}}) &(a_{23}-\frac{a_{21}a_{13}}{a_{11}}) & (a_{24}-\frac{a_{21}a_{14}}{a_{11}})\\
0& (a_{32}-\frac{a_{31}a_{12}}{a_{11}})& (a_{33}-\frac{a_{31}a_{13}}{a_{11}})& (a_{34}-\frac{a_{31}a_{14}}{a_{11}})\\
0&(a_{42}-\frac{a_{41}a_{12}}{a_{11}}) &(a_{43}-\frac{a_{41}a_{13}}{a_{11}}) & (a_{44}-\frac{a_{41}a_{14}}{a_{11}}) \\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           y_1\\
                           w_2^{(2)}\\
                           w_3^{(2)} \\
                           w_4^{(2)}\\
                      \end{array} \right).
\]
or 
\begin{eqnarray}
 b_{11}x_1 +b_{12}x_2 +b_{13}x_3 + b_{14}x_4=&y_1 \nonumber \\
 a^{(2)}_{22}x_2 + a^{(2)}_{23}x_3 + a^{(2)}_{24}x_4=&w^{(2)}_2 \nonumber \\
 a^{(2)}_{32}x_2 + a^{(2)}_{33}x_3 + a^{(2)}_{34}x_4=&w^{(2)}_3 \nonumber \\
 a^{(2)}_{42}x_2 + a^{(2)}_{43}x_3 + a^{(2)}_{44}x_4=&w^{(2)}_4, \nonumber \\
\end{eqnarray}
with the new coefficients 
\[
   b_{1k} = a_{1k}^{(1)} \hspace{0.5cm} k=1,\dots,n,
\]
where each $a_{1k}^{(1)}$ is equal to the original $a_{1k}$ element. The other coefficients are
\[
   a_{jk}^{(2)} = a_{jk}^{(1)}-\frac{a_{j1}^{(1)}a_{1k}^{(1)}}{a_{11}^{(1)}} \hspace{0.5cm} j,k=2,\dots,n,
\]
with a new right-hand side given by 
\[
   y_{1}=w_1^{(1)}, \hspace{0.1cm} w_j^{(2)} =w_j^{(1)}-\frac{a_{j1}^{(1)}w_1^{(1)}}{a_{11}^{(1)}} \hspace{0.5cm} j=2,\dots,n.
\]
We have also set $w_1^{(1)}=w_1$, the original vector element. 
We see that the system of unknowns $x_1,\dots,x_n$ is transformed into an $(n-1)\times (n-1)$ problem.

This step is called forward substitution.
Proceeding with these substitutions, we obtain the 
general expressions for the new coefficients 
\[
   a_{jk}^{(m+1)} = a_{jk}^{(m)}-\frac{a_{jm}^{(m)}a_{mk}^{(m)}}{a_{mm}^{(m)}} \hspace{0.5cm} j,k=m+1,\dots,n,
\]
with $m=1,\dots,n-1$ and a 
right-hand side given by 
\[
   w_j^{(m+1)} =w_j^{(m)}-\frac{a_{jm}^{(m)}w_m^{(m)}}{a_{mm}^{(m)}} \hspace{0.5cm} j=m+1,\dots,n.
\]
This set of $n-1$ elimations leads us to Eq.~(\ref{eq:gaussbacksub}), which is solved by back substitution. 
If the arithmetics is exact and the matrix ${\bf A}$ is not singular, then the computed answer will be exact.
However, as discussed in the two preceeding chapters, computer arithmetics is not exact.  
We will always have to cope 
with truncations and possible losses of precision. Even though the matrix elements along the diagonal are not zero,
numerically small numbers may appear and subsequent divisions may lead to large numbers, which, if added
to a small number may yield losses of precision. Suppose for example that our first division in $(a_{22}-a_{21}a_{12}/a_{11})$
results in $-10^{7}$, that is  $a_{21}a_{12}/a_{11}$. Assume also that $a_{22}$ is one. We are then 
adding $10^7+1$. With single precision this results in $10^7$. Already at this stage we see the potential for
producing wrong results.

The solution to this set of problems is called pivoting, and we distinguish between partial and full pivoting.
Pivoting means that if small values (especially zeros) 
do appear on the diagonal we remove them by 
rearranging the matrix and vectors by permuting rows and columns. 
As a simple example, let us assume that at some stage during a calculation we have
the following set of linear equations
\[
\left(\begin{array}{cccc}
                           1& 3 & 4& 6\\
                           0& 10^{-8} & 198& 19\\
                           0& -91 & 51& 9\\
                           0& 7 & 76& 541\\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           y_1\\
                           y_2\\
                           y_3 \\
                           y_4\\
                      \end{array} \right).
\]
The element at row $i=2$ and column $2$ is $10^{-8}$ and may cause problems for us in the 
next forward substitution. The element $i=2,j=3$ is the largest in the second row and the element $i=3,j=2$ 
is the largest in the third row. The small element can be removed by rearranging 
the rows and/or columns to bring a 
larger value into the $i=2,j=2$ element.

In partial or column pivoting, we rearrange the rows of the matrix and 
the right-hand side to bring the numerically largest value in the column onto the diagonal. 
For our example matrix the largest value of column two is in element $i=3,j=2$ and we interchange rows 2 and 3 to give
\[
\left(\begin{array}{cccc}
                           1& 3 & 4& 6\\
                           0& -91 & 51& 9\\
                           0& 10^{-8} & 198& 19\\
                           0& 7 & 76& 541\\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_2\\
                           x_3 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           y_1\\
                           y_3 \\
                           y_2\\
                           y_4\\
                      \end{array} \right).
\]
Note that our unknown variables $x_i$ remain in the same order which simplifies 
the implementation of this procedure. The right-hand side vector, however, 
has been rearranged. Partial pivoting may be implemented for every step 
of the solution process, or only when the diagonal values are sufficiently 
small as to potentially cause a problem. Pivoting for every step will lead to 
smaller errors being introduced through numerical inaccuracies, 
but the continual reordering will slow down the calculation.  


The philosophy behind full pivoting is much the same as that behind partial pivoting. 
The main difference is that the numerically largest value in the column 
or row containing the value to be replaced. In our example above
the magnitude of element $i=2,j=3$ is the greatest in row 2 or column 2. We could rearrange the columns  
in order to bring this element onto the diagonal. 
This will also entail a rearrangement of the solution vector $x$. The rearranged system becomes, interchanging columns
two and three,
\[
\left(\begin{array}{cccc}
                           1& 6 & 3& 4\\
                           0& 198&10^{-8}& 19\\
                           0 & 51& -91& 9\\
                           0 & 76& 7& 541\\
                      \end{array} \right)\left(\begin{array}{c}
                           x_1\\
                           x_3\\
                           x_2 \\
                           x_4  \\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           y_1\\
                           y_2\\
                           y_3 \\
                           y_4\\
                      \end{array} \right).
\]
The ultimate degree of accuracy can be provided by rearranging both rows and columns so that the numerically 
largest value in the submatrix not yet processed is brought onto the diagonal. 
This process may be undertaken for every step, or only when the value on the diagonal 
is considered too small relative to the other values in the matrix. In our case, the matrix element at $i=4,j=4$ is the largest.
We could here interchange rows two and four and then columns two and four to bring this matrix element at 
the diagonal position $i=2,j=2$. When interchanging columns and rows, one needs to keep track of all
permutations performed. 
Partial and full pivoting are discussed in most texts on numerical linear algebra. For an in-depth
discussion we recommend again the text of Golub and  Van Loan \cite{golub1996}, in particular chapter three. 
See also the discussion of chapter two in Ref.~\cite{numrec}.
The library functions you end up using, be it via Matlab, the library included with this text or other ones,
do all include pivoting. 
 
If it is not possible to rearrange the columns or rows to remove a zero from the diagonal, 
then the matrix A is singular and no solution exists. 

Gaussian elimination requires however many floating point operations. An $n\times n$ matrix requires for the
simultaneous solution of a set of $r$ different right-hand sides, a total of $n^3/3+rn^2-n/3$ multiplications.
Adding the cost of additions, we end up with $2n^3/3+O(n^2)$  floating point operations, see Kress \cite{kress}
for a proof.  An $n\times n$ matrix of dimensionalty $n=10^3$ requires, on a modern PC with a processor
that allows for something like $10^9$ floating point operations per second (flops), approximately one second.
If you increase the size of the matrix to $n=10^4$ you need 1000 seconds, or roughly 
16 minutes.

Although the direct Gaussian elmination algorithm allows you to compute the determinant of ${\bf A}$ via the 
product of the diagonal matrix elements of the triangular matrix, it is seldomly used in normal applications.
The more practical elimination is provided by what is called lower and upper decomposition.  
Once decomposed, one can use this matrix to solve many other linear systems which use the 
same matrix ${\bf A}$, viz with different right-hand sides. With an LU decomposed matrix, the number of 
floating point operations for solving a set of linear equations scales as $O(n^2)$. One should however note
that to obtain the LU decompsed matrix requires roughly $O(n^3)$ floating point operations. 
Finally, LU decomposition  allows for an efficient computation of the inverse of ${\bf A}$. 

\subsection{LU Decomposition of a Matrix}\label{subsec:ludecomp}
%

A frequently used form of Gaussian elimination is L(ower)U(pper) factorization also known as LU Decomposition 
or Crout or Dolittle factorisation. 
In this section we describe how one can decompose a matrix
$A$ in terms of a matrix $L$ with elements only below the diagonal
(and thereby the naming lower) and a matrix $U$ which contains
both the diagonal and matrix elements above the diagonal
(leading to the labelling upper). 
Consider again the matrix ${\bf A}$ given in Eq.~(\ref{eq-1}).
The LU decomposition method means that we can rewrite
this matrix as the product of two matrices ${\bf L}$ and ${\bf U}$
where 
\begin{equation}
\label{eq3}
    {\bf A}= {\bf LU} = \left(\begin{array}{cccc}
                          a_{11} & a_{12} & a_{13} & a_{14} \\
                          a_{21} & a_{22} & a_{23} & a_{24} \\
                          a_{31} & a_{32} & a_{33} & a_{34} \\
                          a_{41} & a_{42} & a_{43} & a_{44} 
                      \end{array} \right)
                      = \left( \begin{array}{cccc}
                              1  & 0      & 0      & 0 \\
                          l_{21} & 1      & 0      & 0 \\
                          l_{31} & l_{32} & 1      & 0 \\
                          l_{41} & l_{42} & l_{43} & 1 
                      \end{array} \right) 
                        \left( \begin{array}{cccc}
                          u_{11} & u_{12} & u_{13} & u_{14} \\
                               0 & u_{22} & u_{23} & u_{24} \\
                               0 & 0      & u_{33} & u_{34} \\
                               0 & 0      &  0     & u_{44} 
             \end{array} \right).
\end{equation} 

LU decomposition forms the backbone of other algorithms in linear algebra, such as the
solution of linear equations given by
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4.  \nonumber
\end{eqnarray}
The above set of equations is conveniently solved by using LU decomposition as an intermediate step,
see the next subsection for more details on how to solve linear equations with an LU decomposed
matrix. 

The matrix ${\bf A}\in \mathbb{R}^{n\times n}$ has an LU factorization if the determinant 
is different from zero. If the LU factorization exists and ${\bf A}$ is non-singular, then the LU factorization
is unique and the determinant is given by 
\[
det\{{\bf A}\}
  = u_{11}u_{22}\dots u_{nn}.
\]
For a proof of this statement, see chapter 3.2 of Ref.~\cite{golub1996}.

The algorithm for obtaining $L$ and $U$ is actually quite simple.
We start always with the first column. In our simple ($4\times 4$) case
we obtain then the following equations for the first column
\[
      \begin{array}{ccc} a_{11} &=& u_{11}\\
                                 a_{21} & = &l_{21}u_{11} \\
                                 a_{31} & = &l_{31}u_{11} \\
                                 a_{41} & = &l_{41}u_{11},
             \end{array}
\]
which determine the elements $u_{11}$, $l_{21}$, $l_{31}$ and $l_{41}$ in 
{\bf L} and {\bf U}. Writing out the equations for the second
column we get
\[
      \begin{array}{ccc} a_{12} &=& u_{12}\\
                                 a_{22} & = &l_{21}u_{12}+u_{22} \\
                                 a_{32} & = &l_{31}u_{12}+l_{32}u_{22} \\
                                 a_{42} & = &l_{41}u_{12} +l_{42}u_{22}.
             \end{array}
\]

Here the unknowns are $u_{12}$, $u_{22}$, $l_{32}$ and $l_{42}$
which can all be evaluated by means of the results from the
first column and the elements  of {\bf A}.
Note an important feature.
When going from the first to the second column we do not need any
further information from  the matrix elements $a_{i1}$.
This is a general property throughout the whole algorithm.
Thus the memory locations for the matrix {\bf A} can be used to
store the calculated matrix elements of {\bf L} and {\bf U}.
This saves memory. 

We can generalize this procedure into three  equations  
%
\begin{eqnarray} 
 i < j: \quad l_{i1}u_{1j}+l_{i2}u_{2j} +\dots + l_{ii}u_{ij}=&a_{ij} \nonumber\\
 i = j: \quad l_{i1}u_{1j}+l_{i2}u_{2j} +\dots + l_{ii}u_{jj}=&a_{ij} \nonumber\\
 i > j: \quad l_{i1}u_{1j}+l_{i2}u_{2j} +\dots + l_{ij}u_{jj}=&a_{ij} \nonumber
\end{eqnarray}
%
which gives the following algorithm:\\
Calculate the elements in {\bf L} and {\bf U} columnwise starting with
column one. For each column $(j)$:
%
\begin{itemize}
% 
\item Compute the first element $u_{1j}$ by 
%
\[
          u_{1j} = a_{1j}.
\]
%
%
\item Next, we calculate all elements  $u_{ij}, i = 2, \ldots, j-1$
%
\[
    u_{ij} = a_{ij} -  \sum_{k=1}^{i-1}l_{ik}u_{kj}.
\]
%
\item Then calculate the diagonal element $u_{jj}$ 
%
\begin{equation}
\label{eq6}
   u_{jj} = a_{jj} - \sum_{k=1}^{j-1}l_{jk}u_{kj}.
\end{equation} 
% 
\item Finally, calculate the elements $l_{ij}, i > j$
%
\begin{equation}
\label{eq7}
      l_{ij} = 
      \frac{1}{u_{jj}}\left(a_{ij}-\sum_{k=1}^{i-1}l_{ik}u_{kj}\right),
\end{equation} 
%
\end{itemize}
%
The algorithm is known as Doolittle's algorithm since the diagonal matrix elements of ${\bf L}$ 
are $1$. For the case where the diagonal elements of ${\bf U}$ 
are $1$, we have what is called Crout's algorithm. For the case where 
${\bf U} = {\bf L}^T$ so that $u_{ii}=l_{ii}$ for $ 1 \leq i \leq n$ we can use what is called the Cholesky 
factorization algorithm. In this case the matrix ${\bf A}$ has to fulfill several features; namely, it should be real, symmetric and positive definite. A matrix is positive definite if 
the quadratic form ${\bf x}^T{\bf A}{\bf x} > 0$. Establishing this feature is not easy since it 
implies the use of an arbitrary  vector ${\bf x} \neq 0$. If the matrix is positive definite and
symmetric, its eigenvalues are always real and positive.  We discuss the Cholesky factorization below.

A crucial point in the LU decomposition is obviously the case
where $u_{jj}$ is close to or equals zero, a case which can
lead to serious problems. 
Consider the following simple $2\times 2$ example taken from Ref.~\cite{trefethen}
\[
    {\bf A}= \left( \begin{array}{cc}
                              0  & 1 \\
                              1  & 1
                      \end{array} \right). 
\]
The algorithm discussed above fails immediately, the first step simple states
that $u_{11} = 0$. We could change slightly the above matrix by replacing $0$ with 
$10^{-20}$ resulting in 
\[
    {\bf A}= \left( \begin{array}{cc}
                              10^{-20} & 1 \\
                              1  & 1
                      \end{array} \right), 
\]
yielding 
\[
      \begin{array}{ccc} u_{11} &=& 10^{-20} \\
                                 l_{21} & = & 10^{20} \\
             \end{array}
\]
and $u_{12} = 1$ and
\[
   u_{22} = a_{11} - l_{21}=1-10^{20},
\]
we obtain
\[
    {\bf L}= \left( \begin{array}{cc}
                              1 & 0 \\
                              10^{20}  & 1
                      \end{array} \right), 
\]
and 
\[
    {\bf U}= \left( \begin{array}{cc}
                              10^{-20} & 1 \\
                              0  & 1-10^{20}
                      \end{array} \right), 
\]
With the change from 0 to a small number like $10^{-20}$ we see that the LU decomposition is now stable,
but it is not backward stable. What do we mean by that?
First we note that  
the matrix ${\bf U}$ has an element $u_{22}=1-10^{20}$. Numerically, since we do have a limited
precision, which for double precision is approximately $\epsilon_M\sim 10^{-16}$ 
it means that this number is approximated in the machine  as $u_{22}\sim -10^{20}$ resulting in a machine
representation of the matrix as 
\[
    {\bf U}= \left( \begin{array}{cc}
                              10^{-20} & 1 \\
                              0  & -10^{20}
                      \end{array} \right). 
\]
If we multiply the matrices ${\bf LU}$ we have
\[
    \left( \begin{array}{cc}
                              1 & 0 \\
                              10^{20}  & 1
                      \end{array} \right)\left( \begin{array}{cc}
                              10^{-20} & 1 \\
                              0  & -10^{20}
                      \end{array} \right)=\left( \begin{array}{cc}
                              10^{-20} & 1 \\
                              1 & 0
                      \end{array} \right)
 \neq {\bf A}. 
\]
We do not get back the original matrix ${\bf A}$!


The solution is pivoting
(interchanging rows in this case) around the largest element in a column $j$.
Then we are actually decomposing a rowwise permutation of
the original matrix $\bf {A}$. The key point to notice is that
Eqs.~(\ref{eq6}) and  (\ref{eq7}) are equal except for the case that we divide
by $u_{jj}$ in the latter one. The upper limits are always the same
$k=j-1(=i-1)$. This means that we do not have to choose the diagonal
element $u_{jj}$ as the one which happens to fall along the 
diagonal in the first instance.  Rather, we could promote one of 
the undivided $l_{ij}$'s in the column $i=j+1, \dots N$ 
to become the diagonal of $U$. The partial pivoting 
in Crout's or Doolittle's methods means then that we choose the largest 
value for $u_{jj}$ (the pivot element) and then do the divisions
by that element. Then we need to keep track of all permutations performed. 
For the above matrix ${\bf A}$ it would have sufficed to interchange the two rows and start
the LU decomposition with 
\[
    {\bf A}= \left( \begin{array}{cc}
  
                              1  & 1\\
                            0  & 1  
                    \end{array} \right). 
\]



The error which is done in the LU decomposition of an $n\times n$ matrix if no zero pivots are encountered
is given by, see chapter 3.3 of Ref.~\cite{golub1996},
\[
    {\bf LU} = {\bf A} + {\bf H},
\]
with 
\[
   |{\bf H}| \leq 3(n-1) {\bf u} \left(|{\bf A}|+|{\bf L}||{\bf U}|\right)+O({\bf u}^2),
\]
with $|{\bf H}|$ being the absolute value of a matrix and $ {\bf u}$ is the error done in
representing the matrix elements of the matrix ${\bf A}$ as floating points in a machine 
with a given precision $\epsilon_M$, viz.~every matrix element of $ {\bf u}$ is
\[
     |fl(a_{ij}) -a_{ij}|\leq u_{ij},
\]
with $|u_{ij}| \leq \epsilon_M$ resulting in
\[
     |fl({\bf A}) -{\bf A}|\leq {\bf u}|{\bf A}|.
\] 


The programs  which perform the above described LU decomposition are called as follows
%
\begin{svgraybox}
\begin{center} 
{C++: \hspace{1cm} ludcmp(double $**$a, int n, int $*$indx, double $*$d)}\\
{Fortran: \hspace{0.5cm} CALL lu\_decompose(a, n, indx, d)}
\end{center}
Both the C++ and Fortran 90/95 programs receive as input the matrix to be LU decomposed.
In C++ this is given  by the double pointer \lstinline{ **a}. Further, both functions need
the size of the matrix $n$. It returns the variable $d$, 
which is $\pm 1$ depending on whether we have an even or odd number of row interchanges, 
a pointer $indx$ that records the row permutation which has been effected and the LU decomposed matrix. Note that the original matrix is destroyed.
\end{svgraybox}

\subsubsection{Cholesky's Factorization}

If the matrix $A$ is real, symmetric and positive definite, then
it has  a unique factorization (called Cholesky factorization)
\[
   A = LU = LL^T
\]
where $L^T$ is the upper matrix, implying that
\[
  L^T_{ij} = L_{ji}.
\]
The algorithm for the Cholesky decomposition
is a special case of the general LU-decomposition algorithm.
The algorithm of this decomposition is as follows
\begin{itemize}
\item Calculate the diagonal element $L_{ii}$ by setting up a loop 
for $i=0$ to $i=n-1$ (C++ indexing of matrices and vectors)
\[
   L_{ii} = \left(A_{ii} - \sum_{k=0}^{i-1}L_{ik}^2\right)^{1/2}.
\]
%
\item within the loop over $i$, introduce a new loop which goes 
from $j=i+1$ to $n-1$ and calculate 
%
\[
      L_{ji} =
      \frac{1}{L_{ii}}\left(A_{ij}-\sum_{k=0}^{i-1}L_{ik}l_{jk}\right).
\]
\end{itemize}
For the Cholesky algorithm we have always that $L_{ii} > 0$ and the problem
with exceedingly large matrix elements does not appear and hence there is no
need for pivoting.

To decide whether a matrix is positive definite or not needs some careful analysis. To find
criteria for positive definiteness, one needs two statements from matrix theory, see Golub and Van Loan \cite{golub1996}
for examples. First, the leading principal submatrices of a positive definite matrix are positive definite and non-singular
and secondly a matrix is positive definite if and only if it has an ${\bf LDL}^T$ factorization with positive diagonal elements
only in the diagonal matrix ${\bf D}$. A positive definite matrix has to be symmetric and have only positive eigenvalues.

The easiest way therefore to test whether a matrix is positive definite or not is to solve the eigenvalue problem 
${\bf Ax}=\lambda {\bf x}$ and check that all eigenvalues are positive.
 
\subsection{Solution of Linear Systems of Equations}\label{subsec:lineq}
%
With the LU decomposition it is rather 
simple to solve a system of linear equations
%
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 + a_{14}x_4=&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + a_{24}x_4=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + a_{34}x_4=&w_3 \nonumber \\
a_{41}x_1 + a_{42}x_2 + a_{43}x_3 + a_{44}x_4=&w_4. \nonumber
\end{eqnarray}
%
This can be written in matrix form as 
\[
   {\bf Ax}={\bf w}.
\]
%
where ${\bf A}$ and ${\bf w}$ are known and we have to solve for
${\bf x}$. Using the LU dcomposition we write 
%
\begin{equation}
%\label{eq4}
   {\bf A} {\bf x} \equiv {\bf L} {\bf U} {\bf x} ={\bf w}.
\end{equation}
%
This equation can be calculated in two steps
%
\begin{equation}
  {\bf L} {\bf y} = {\bf w}; \hspace*{2cm} {\bf Ux}={\bf y}.
  \label{eq:byw}
\end{equation}
%
To show that this is correct we use to the LU decomposition
to rewrite our system of linear equations as
\[
   {\bf LUx}={\bf w},
\]
and since the determinat of ${\bf L}$ is equal to 1 (by construction
since the diagonals of ${\bf L}$ equal 1) we can use the inverse of
${\bf L}$ to obtain 
\[
   {\bf Ux}={\bf L^{-1}w}={\bf y},
\]
which yields the intermediate step 
\[
   {\bf L^{-1}w}={\bf y}
\]
and multiplying with ${\bf L}$ on both sides we reobtain Eq.\ 
(\ref{eq:byw}). As soon as we have ${\bf y}$ we can obtain ${\bf x}$
through ${\bf Ux}={\bf y}$. 

For our four-dimentional example this takes the form 
%
\begin{eqnarray} 
 y_1=&w_1 \nonumber\\
l_{21}y_1 + y_2=&w_2\nonumber \\
l_{31}y_1 + l_{32}y_2 + y_3 =&w_3\nonumber \\
l_{41}y_1 + l_{42}y_2 + l_{43}y_3 + y_4=&w_4. \nonumber
\end{eqnarray}
%
and 
%
\begin{eqnarray} 
 u_{11}x_1 +u_{12}x_2 +u_{13}x_3 + u_{14}x_4=&y_1 \nonumber\\
u_{22}x_2 + u_{23}x_3 + u_{24}x_4=&y_2\nonumber \\
u_{33}x_3 + u_{34}x_4=&y_3\nonumber \\
u_{44}x_4=&y_4  \nonumber
\end{eqnarray}
%
This example shows the basis for the algorithm
needed to solve the set of $n$ linear equations. 
The algorithm goes as follows
%
\begin{svgraybox}
\begin{itemize}
\item Set up the matrix {\bf A} and the vector {\bf w}
      with their correct dimensions. This determines the dimensionality
      of the unknown vector {\bf x}.
\item Then LU decompose the matrix {\bf A} through a call to
      the function
      % 
      \begin{center}
       \begin{tabular}{ll} 
         C++:       &{ludcmp(double a, int n, int indx, double \&d)}\\
         Fortran: &{CALL lu\_decompose(a, n, indx, d)}
       \end{tabular}
      \end{center}
      %
      This functions returns the LU decomposed
      matrix {\bf A}, its determinant and the vector indx which keeps track 
     of the number of interchanges of  rows. If the determinant is zero, 
     the solution is malconditioned.
\item Thereafter you call the function
      %
      \begin{center}
       \begin{tabular}{ll}
         C++: &{lubksb(double a, int n, int indx, double w)}\\
         Fortran: &{CALL lu\_linear\_equation(a, n, indx, w)}
       \end{tabular}
      \end{center}
      %
      which uses the
      LU decomposed matrix {\bf A} and the vector {\bf w} and returns {\bf x}
      in the same place as {\bf w}. Upon exit the original content
      in {\bf w} is destroyed. If you wish to keep this information, you should make
      a backup of it in your calling function.
\end{itemize}
\end{svgraybox}

\subsection{Inverse of a Matrix and the Determinant}\label{subsec:inverse}
%
The basic definition of the determinant of {\bf A} is 
%
\[
det\{{\bf A}\}
  = \sum_{p} (-1)^{p} a_{1p_1} \cdot a_{2p_2} \cdots a_{np_{n}},
\]
where the sum runs over all permutations $p$ of the indices
$1,2,\ldots,n$, altogether $n!$ terms. To calculate the inverse
of {\bf A} is a formidable task. Here we have to calculate {\sl the
complementary cofactor $a^{ij}$} of each element $a_{ij}$ which is the
$(n - 1)$determinant
obtained by striking out the row $i$ and column $j$ in which the
element $a_{ij}$ appears. The inverse of {\bf A} is then constructed as
the transpose  of a matrix with the elements $(-)^{i + j}
a^{ij}$. This
involves a calculation of $n^2$ determinants using the formula above.
A simplified method is highly needed.

With the LU decomposed matrix {\bf A} in Eq.~(\ref{eq3})
it is rather easy to find the determinant
% 
\[
   det\{{\bf A}\}=det\{{\bf L}\}\times det\{{\bf U}\} = det\{{\bf U}\},
\]
%
since the diagonal elements of {\bf L} equal 1. Thus the determinant
can be written
%
\[
   det\{{\bf A}\} =\prod_{k=1}^Nu_{kk}.
\]

The inverse is slightly more difficult. However, with an LU decomposed matrix this reduces to
solving a set of linear equations. To see this, we recall that if the inverse exists
then
\[
   {\bf A}^{-1}{\bf A}={\bf I},
\]
the identity matrix. With an LU decomposed matrix we can rewrite the last equation as
\[
   {\bf LU}{\bf A}^{-1}={\bf I}.
\]
If we assume that the first column (that is column 1) of the inverse matrix 
can be written as a vector with unknown entries
\[
    {\bf A}_1^{-1}= \left( \begin{array}{c}
  
                              a_{11}^{-1} \\
                              a_{21}^{-1} \\  
                              \dots \\  
                              a_{n1}^{-1} \\  
                    \end{array} \right), 
\]
then we have a linear set of equations
\[
    {\bf LU}\left( \begin{array}{c}
  
                              a_{11}^{-1} \\
                              a_{21}^{-1} \\  
                              \dots \\  
                              a_{n1}^{-1} \\  
                    \end{array} \right) =\left( \begin{array}{c}
                               1 \\
                              0 \\  
                              \dots \\  
                              0 \\  
                    \end{array} \right).
\]
In a similar way we can compute the unknow entries of the second column,
\[
    {\bf LU}\left( \begin{array}{c}
  
                              a_{12}^{-1} \\
                              a_{22}^{-1} \\  
                              \dots \\  
                              a_{n2}^{-1} \\  
                    \end{array} \right) =\left( \begin{array}{c}
                                0 \\
                              1 \\  
                              \dots \\  
                              0 \\  
                    \end{array} \right),
\]
and continue till we have solved all $n$ sets of linear equations.

A calculation of the inverse of a matrix could then be implemented in the following way:
\begin{svgraybox}
\begin{itemize}
\item Set up the matrix to be inverted.
\item Call the LU decomposition function.
\item Check whether the determinant is zero or not.
\item Then solve column by column the sets of linear equations.
\end{itemize}
\end{svgraybox}
%
The following codes compute the inverse of a matrix using either C++ or Fortran
as programming languages. They are both included in the library packages, but we include them explicitely
here as well as two distinct programs which use these functions.
We list first the C++ code.
\lstset{language=c++} 
\begin{lstlisting}[title={\url{http://folk.uio.no/compphys/programs/chapter06/cpp/program1.cpp}}]
/* The function
**                inverse()
** perform a mtx inversion of the input matrix a[][] with
** dimension n. 
*/
void inverse(double **a, int n)
{        
  int          i,j, *indx;
  double       d, *col, **y;

  // allocate space in memory
  indx = new int[n];
  col  = new double[n];
  y    = (double **) matrix(n, n, sizeof(double)); 
  // first we need to LU decompose the matrix
  ludcmp(a, n, indx, &d); 
  // find inverse of a[][] by columns 
  for(j = 0; j < n; j++) {
    // initialize right-side of linear equations 
    for(i = 0; i < n; i++) col[i] = 0.0;
    col[j] = 1.0;
    lubksb(a, n, indx, col);
    // save result in y[][] 
    for(i = 0; i < n; i++) y[i][j] = col[i];
  }  
  // return the inverse matrix in a[][] 

  for(i = 0; i < n; i++) {
    for(j = 0; j < n; j++) a[i][j] = y[i][j];
  } 
  free_matrix((void **) y);     // release local memory 
  delete [] col;
  delete []indx;

}  // End: function inverse()

\end{lstlisting}
We first need to LU decompose the matrix. Thereafter we solve linear equations 
by using the back substitution method calling the function {\bf lubksb}
and obtain finally the inverse matrix. 

An example of a C++ function which calls this function is also given in the following program and reads
\lstset{language=c++} 
\begin{lstlisting}[title={\url{http://folk.uio.no/compphys/programs/chapter06/cpp/program1.cpp}}]
//  Simple matrix inversion example
#include <iostream>
#include <new>
#include <cstdio>
#include <cstdlib>
#include <cmath>
#include <cstring>
#include    "lib.h"

using namespace std;

/* function declarations */

void inverse(double **, int);
/*
** This program sets up a simple 3x3 symmetric matrix
** and finds its determinant and inverse
*/

int main()
{
  int          i, j, k, result, n = 3;
  double       **matr, sum,  
    a[3][3]   = { {1.0, 3.0, 4.0},
		  {3.0, 4.0, 6.0},
		  {4.0, 6.0, 8.0}};
  // memory for  inverse matrix 
  matr = (double **) matrix(n, n, sizeof(double));   
  // various print statements in the original code are omitted

  inverse(matr, n);     // calculate and return inverse matrix  
  ....
  return 0;
} // End: function main() 
\end{lstlisting} 
In order to use the program library you need to include the {\bf lib.h} file using the 
\lstinline{#include    "lib.h"} statement.
This function utilizes the library function {\bf matrix} and {\bf free\_matrix} to allocate
and free memory during execution. The matrix $ a[3][3]$ is set at compilation time. 
Alternatively, you could have used either Blitz++ or Armadillo.

The corresponding Fortran program for the inverse of a matrix reads
\lstset{language=[90]Fortran} 
\begin{lstlisting}[title={\url{http://folk.uio.no/compphys/programs/FYS3150/f90 library/f90lib.f90}}]
  !
  !            Routines to do mtx inversion, from Numerical
  !            Recipes, Teukolsky et al. Routines included
  !            below are MATINV, LUDCMP and LUBKSB. See chap 2
  !            of Numerical Recipes for further details
  !
  SUBROUTINE matinv(a,n, indx, d)
    IMPLICIT NONE
    INTEGER, INTENT(IN) :: n
    INTEGER :: i, j
    REAL(DP), DIMENSION(n,n), INTENT(INOUT)  :: a
    REAL(DP), ALLOCATABLE :: y(:,:)
    REAL(DP) :: d
    INTEGER, , INTENT(INOUT) :: indx(n)

    ALLOCATE (y( n, n))
    y=0.
    !     setup identity matrix
    DO i=1,n
       y(i,i)=1.
    ENDDO
    !     LU decompose the matrix just once
    CALL  lu_decompose(a,n,indx,d)

    !     Find inverse by columns
    DO j=1,n
       CALL lu_linear_equation(a,n,indx,y(:,j))
    ENDDO
    !     The original matrix a was destroyed, now we equate it with the inverse y 
    a=y
    DEALLOCATE ( y )

  END SUBROUTINE matinv
\end{lstlisting}
The Fortran program {\bf matinv} receives as input the same variables as the 
C++ program and calls the function for LU decomposition {\bf lu\_decompose} and the 
function to solve sets of linear equations {\bf lu\_linear\_equation}. 
The program listed under programs/chapter4/program1.f90 performs the same action as the 
C++ listed above. In order to compile and link these programs it is convenient to
use a so-called {\bf makefile}. Examples of these are found under the same catalogue
as the above programs.
\subsubsection{Scattering Equation and Principal Value Integrals via Matrix Inversion}

In quantum mechanics, it is often common to rewrite Schr\"odinger's equation in momentum space,
after having made a so-called partial wave expansion of the interaction. We will not go into
the details of these expressions but limit ourselves to study the equivalent problem
for so-called scattering states, meaning that the total energy of two 
particles which collide is larger than or equal zero. The benefit of rewriting the equation in momentum space, after having performed a Fourier transformation, is that the coordinate
space equation, being an integro-differantial equation, is transformed into an integral
equation. The latter can be solved by standard matrix inversion techniques.
Furthermore, the results of solving these equation can be related directly to experimental
observables like the scattering phase shifts. The latter tell us how much the incoming two-particle wave function is modified by a collision.
Here we take a more technical stand and consider the technical aspects of solving
an integral equation with a principal value.

For scattering states, $E>0$, the corresponding equation to solve is 
the so-called Lippman-Schwinger equation. This is an integral equation
where we have to deal with the amplitude $R(k,k')$ (reaction matrix) 
defined through the integral equation 
\begin{equation}
    R_l(k,k') = V_l(k,k') +\frac{2}{\pi}{\cal P}
                \int_0^{\infty}dqq^2V_l(k,q)\frac{1}{E-q^2/m}R_l(q,k'),
   \label{eq:ls1}
\end{equation}
where the total kinetic energy of the two 
incoming particles in the center-of-mass system
is 
\begin{equation}
    E=\frac{k_0^2}{m}.
\end{equation}
The symbol ${\cal P}$ indicates that Cauchy's principal-value prescription
is used in order to avoid the singularity arising from the zero of the denominator.
We will discuss below how to solve this problem. Equation (\ref{eq:ls1}) represents
then the problem you will have to solve numerically.  The interaction between the two
particles is given by a partial-wave decomposed version $V_l(k,k')$, where $l$ stands for
a quantum number like the orbital momentum. We have assumed that interaction does not
coupled to partial waves with different orbital momenta. The variables $k$ and $k'$
are the outgoing and incoming relative momenta of the two interacting particles.

The matrix $R_l(k,k')$ relates to the experimental  
the  phase shifts $\delta_l$ through its diagonal elements as
\begin{equation}
     R_l(k_0,k_0)=-\frac{tan\delta_l}{mk_0}, 
     \label{eq:shifts}
\end{equation}
where $m$ is the reduced mass of the interacting particles.  Furthemore, the interaction
between the particles, $V$, carries 

In order to solve the Lippman-Schwinger equation 
in momentum space, we need first to write 
a function which sets up the integration points. 
We need to do that since we are going to approximate the integral
through 
\[
   \int_a^bf(x)dx\approx\sum_{i=1}^Nw_if(x_i),
\]
where we have fixed $N$ integration points through the corresponding weights
$w_i$ and points $x_i$. These points can for example be determined using
Gaussian quadrature.

The principal value in Eq.\ (\ref{eq:ls1}) is rather tricky
to evaluate numerically, mainly since computers have limited
precision. We will here use a subtraction trick often used
when dealing with singular integrals in numerical calculations.
We use the calculus relation  from the previous section
\[
  \int_{-\infty}^{\infty} \frac{dk}{k-k_0} =0,
\]
or
\[
  \int_{0}^{\infty} \frac{dk}{k^2-k_0^2} =0.
\]
We can use this to express a principal values integral
as
\begin{equation}
  {\cal P}\int_{0}^{\infty} \frac{f(k)dk}{k^2-k_0^2} =
  \int_{0}^{\infty} \frac{(f(k)-f(k_0))dk}{k^2-k_0^2},
   \label{eq:trick}
\end{equation}
where the right-hand side is no longer singular at 
$k=k_0$, it is proportional to the derivative $df/dk$,
and can be evaluated numerically as any other integral.

We can then use the trick in Eq.\ (\ref{eq:trick}) to rewrite
Eq.\ (\ref{eq:ls1}) as
\begin{equation}
    R(k,k') = V(k,k') +\frac{2}{\pi}
                \int_0^{\infty}dq
                \frac{q^2V(k,q)R(q,k')-k_0^2V(k,k_0)R(k_0,k')  }
                     {(k_0^2-q^2)/m}.
   \label{eq:ls2}
\end{equation}
We are interested in obtaining $R(k_0,k_0)$, since this is the quantity we want to relate
to experimental data like the phase shifts.

How do we proceed in order to solve Eq.\ (\ref{eq:ls2})?
\begin{enumerate}
  \item  Using the mesh points $k_j$ and the weights $\omega_j$,
         we can rewrite Eq.\ (\ref{eq:ls2}) as
\begin{equation}
          R(k,k') = V(k,k') +\frac{2}{\pi}
          \sum_{j=1}^N\frac{\omega_jk_j^2V(k,k_j)R(k_j,k')}
                           {(k_0^2-k_j^2)/m}
           -\frac{2}{\pi}k_0^2V(k,k_0)R(k_0,k')
          \sum_{n=1}^N\frac{\omega_n}
                           {(k_0^2-k_n^2)/m}.                
          \label{eq:ls3}
\end{equation}
This equation contains now the unknowns $R(k_i,k_j)$
(with dimension $N\times N$) and $R(k_0,k_0)$.
\item 
We can turn Eq.\ (\ref{eq:ls3}) into an equation
with dimension $(N+1)\times (N+1)$ with  an integration domain
which contains the original mesh points $k_j$ for $j=1,N$
and the point which corresponds to the energy $k_0$.
Consider the latter as the 'observable' point.
The mesh points become then $k_j$ for $j=1,n$ and
$k_{N+1}=k_0$. 
\item With these new mesh points we define the matrix
\begin{equation}
      A_{i,j}=\delta_{i,j}-V(k_i,k_j)u_j,
      \label{eq:aeq}
\end{equation}
where $\delta$ is the Kronecker $\delta$
and
\begin{equation}
     u_j=\frac{2}{\pi}
         \frac{\omega_jk_j^2}{(k_0^2-k_j^2)/m}\hspace{1cm}
         j=1,N
\end{equation}
and
\begin{equation}
     u_{N+1}=-\frac{2}{\pi}
          \sum_{j=1}^N\frac{k_0^2\omega_j}{(k_0^2-k_j^2)/m}.
\end{equation}
The first task is then to 
set up the matrix $A$ for a given $k_0$. This is an
$(N+1)\times (N+1)$ matrix. It can be convenient
to have an outer loop which runs over the chosen
observable values for the energy $k_0^2/m$.
{\em Note that all mesh points $k_j$ for $j=1,N$ must be
different from $k_0$. Note also that
$V(k_i,k_j)$ is an
$(N+1)\times (N+1)$ matrix}.
\item
  With the matrix $A$ we can rewrite Eq.\ (\ref{eq:ls3}) 
  as a matrix problem of dimension $(N+1)\times (N+1)$.
  All matrices $R$, $A$ and $V$ have this dimension
  and we get
\begin{equation}
    A_{i,l}R_{l,j}=V_{i,j},
\end{equation} 
or just
\begin{equation}
    AR=V.
    \label{eq:final1}
\end{equation} 
\item Since we already have defined $A$ and $V$
(these are stored as $(N+1)\times (N+1)$ matrices) 
Eq.\ (\ref{eq:final1}) involves only the unknown
$R$. We obtain it by matrix inversion, i.e.,
\begin{equation}
    R=A^{-1}V.
    \label{eq:final2}
\end{equation} 
Thus, to obtain $R$, we need to set up the matrices
$A$ and $V$ and invert the matrix $A$. With the  inverse $A^{-1}$ we
perform
a matrix multiplication with $V$ and obtain  $R$.
\end{enumerate}

With $R$ we can in turn evaluate the phase shifts
by noting that 
\begin{equation}
      R(k_{N+1},k_{N+1})=R(k_0,k_0),
\end{equation}
and we are done.

\subsubsection{Inverse of the Vandermonde Matrix}
In chapter \ref{chap:differentiate} we discussed how to interpolate  a function
$f$ which is known only at $n+1$ points $x_0, x_1, x_2,\dots, x_n$ with corresponding
values $f(x_0), f(x_1), f(x_2),\dots, f(x_n)$.  
The latter is often a typical outcome of a large scale computation  or from an experiment.
In most cases in the sciences we do not have a closed-form expression  for a function $f$.
The function is only known at specific points.

We seek a functional form for a 
function $f$ which passes through the above pairs of values
\[ 
(x_0,f(x_0)),(x_1,f(x_1)),(x_2,f(x_2)),\dots, (x_n,f(x_n)).
\]
This is normally achieved by expanding the function $f(x)$ in terms of well-known
polynomials $\phi_i(x)$, such as Legendre, Chebyshev, Laguerre etc.  The function is then
approximated by a polynomial of degree $n$  $p_n(x)$ 
\[
    f(x) \approx p_n(x) = \sum_{i=0}^n a_i  \phi_i(x),
\]
where $a_i$ are unknown coefficients and $\phi_i(x)$ are a priori well-known functions. 
The simplest possible case is to assume  that  $\phi_i(x) = x^i$, resulting in an 
approximation 
\[
    f(x) \approx  a_0 + a_1 x  +a_2x^2+\dots+a_nx^n. 
\]
Our function is known at the points 
$n+1$ points $x_0, x_1, x_2,\dots, x_n$, leading to $n+1$ equations of the type
\[
    f(x_i) \approx  a_0 + a_1 x_i  +a_2x_i^2+\dots+a_nx_i^n. 
\]
We can then obtain the unknown coefficients by rewriting our problem as 
\[
 \left(\begin{array}{cccccc}
                           1& x_0 & x_0^2 &\dots   & \dots &x_0^n \\
                           1& x_1 & x_1^2 &\dots   & \dots &x_1^n \\
                           1& x_2 & x_2^2 &\dots   & \dots &x_2^n \\
                           1& x_3 & x_3^2 &\dots   & \dots &x_3^n \\
                           \dots& \dots &\dots &\dots   & \dots &\dots \\
                           1& x_n & x_n^2 &\dots   & \dots &x_n^n \\
                      \end{array} \right)
 \left(\begin{array}{c}
                           a_0 \\
                           a_1 \\
                           a_2 \\
                           a_3 \\
                           \dots \\
                           a_n \\
                      \end{array} \right)  =    \left(\begin{array}{c}
                           f(x_0) \\
                           f(x_1) \\
                           f(x_2) \\
                           f(x_3) \\
                           \dots \\
                           f(x_n) \\
                      \end{array} \right),
\]
an expression which can be rewritten in a more compact form as 
\[
    {\bf X} {\bf a} = {\bf f},
\]
with 
\[
     {\bf X} =  \left(\begin{array}{cccccc}
                           1& x_0 & x_0^2 &\dots   & \dots &x_0^n \\
                           1& x_1 & x_1^2 &\dots   & \dots &x_1^n \\
                           1& x_2 & x_2^2 &\dots   & \dots &x_2^n \\
                           1& x_3 & x_3^2 &\dots   & \dots &x_3^n \\
                           \dots& \dots &\dots &\dots   & \dots &\dots \\
                           1& x_n & x_n^2 &\dots   & \dots &x_n^n \\
                      \end{array} \right).
\]
This matrix is called a Vandermonde matrix and  
is by definition non-singular since all points $x_i$ are different. The inverse exists
and we can obtain the unknown coefficients by inverting ${\bf X}$, resulting in
\[
     {\bf a} ={\bf X}^{-1} {\bf f}.
\]

Although this algorithm for obtaining an interpolating polynomial which approximates our data set
looks very simple, it is an inefficient algorithm since the computation of the inverse requires $O(n^3)$ 
flops.  The methods we discussed in chapter \ref{chap:differentiate}, together with spline interpolation discussed in the next section, are much more effective from a numerical
point of view.  There is also another subtle point. Although we have a data set 
with $n+1$ points, this does not necessarily mean that our function $f(x)$ is well represented by a 
polynomial of degree $n$. On the contrary, our function $f(x)$ may be a parabola (second-order in $n$),
meaning that we have a large excess of data points.  In such cases a least-square fit  or a spline 
interpolation may be better approaches to represent the function.  Spline interpolation will be discussed in the next section.


\subsection{Tridiagonal Systems of Linear Equations}

We start with the linear set of equations from Eq.~(\ref{eq:simpletriag}), viz 
\[
   {\bf A}{\bf u} = {\bf f},
\]
where ${\bf A}$ is a tridiagonal matrix which we rewrite as 
\[
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_{n-1} & b_n \\
                      \end{array} \right)
\]
where $a,b,c$ are one-dimensional arrays of length $1:n$. 
In the example of Eq.~(\ref{eq:simpletriag}) the arrays $a$ and $c$ are equal, namely $a_i=c_i=-1/h^2$.
We can rewrite Eq.~(\ref{eq:simpletriag}) as
\[
    {\bf Au} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_{n-1} & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           u_1\\
                           u_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           u_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           f_1\\
                           f_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           f_n\\
                      \end{array} \right).
\]
A tridiagonal matrix is a special form of banded matrix where all the elements are zero except for 
those on and immediately above and below the leading diagonal.
The above tridiagonal system   can be written as
\[
  a_iu_{i-1}+b_iu_i+c_iu_{i+1} = f_i,
\]
for $i=1,2,\dots,n$. We see that $u_{-1}$ and $u_{n+1}$ are not required and we can set $a_1=c_n=0$.
In many applications the matrix is symmetric and we have $a_i=c_i$.
The algorithm for solving this set of equations is rather simple and requires two steps only,
a forward substitution and a backward substitution. These steps are also 
common to the algorithms based on
Gaussian elimination that 
we discussed previously. However, due to its simplicity, the number of floating point operations  
is in this
case proportional with $O(n)$ while Gaussian elimination requires $2n^3/3+O(n^2)$ floating point operations.  
In case your system of equations leads to a tridiagonal matrix, it is clearly an overkill to employ
Gaussian elimination or the standard LU decomposition. 
You will encounter several applications involving tridiagonal matrices in our discussion of
partial differential equations in chapter \ref{chap:partial}.

Our algorithm starts with forward substitution with a loop over of the elements $i$ and can be expressed via the 
following piece of code taken from the Numerical Recipe text of Teukolsky {\em et al} \cite{numrec}
 \lstset{language=c++} 
\begin{lstlisting}
   btemp = b[1];
   u[1] = f[1]/btemp;
   for(i=2 ; i <= n ; i++) {  
      temp[i] = c[i-1]/btemp;
      btemp = b[i]-a[i]*temp[i];
      u[i] = (f[i] - a[i]*u[i-1])/btemp; 
   }  
\end{lstlisting}
Note that you should avoid cases with $b_1=0$. If that is the case, you should rewrite the equations
as a set of order $n-1$ with $u_2$ eliminated. 
Finally we perform the backsubstitution leading to the following code
\begin{lstlisting}
   for(i=n-1 ; i >= 1 ; i--) {  
      u[i] -= temp[i+1]*u[i+1];
   }  
\end{lstlisting}
Note that our sums start with $i=1$ and that one  should avoid cases with $b_1=0$. If that is the case, you should rewrite the equations
as a set of order $n-1$ with $u_2$ eliminated. However, a tridiagonal matrix problem is not a guarantee that we
can find a solution. The matrix ${\bf A}$ which rephrases a second derivative in a discretized form
\[
    {\bf A} = \left(\begin{array}{cccccc}
                          2 & -1 & 0 & 0  &0  & 0\\
                          -1 & 2 & -1 &0 &0 &0 \\
                          0 & -1 & 2 & -1 & 0& 0 \\
                          0 & \dots   & \dots & \dots   &\dots & \dots \\
                          0 &0   &0  &-1  &2& -1 \\
                          0 &  0  &0  &0   &-1 & 2 \\
                      \end{array} \right),
\]
fulfills the condition of a weak dominance of the diagonal, with
$|b_1| > |c_1|$, $|b_n| > |a_n|$ and  $|b_k| \ge |a_k|+|c_k|$ for $k=2,3,\dots,n-1$.   
This is a relevant but not sufficient condition to guarantee that the matrix ${\bf A}$ yields a solution to a linear
equation problem. The matrix needs also to be irreducible. A tridiagonal irreducible matrix means that all the elements $a_i$ and
$c_i$ are non-zero. If these two conditions are present, then ${\bf A}$ is nonsingular and has a unique LU decomposition.

We can obviously extend our boundary value problem to include a first derivative as well
\[
  -\frac{d^2u(x)}{dx^2}+ g(x)\frac{du(x)}{dx}+h(x)u(x)= f(x),
\]
with $x\in [a,b]$ and with boundary conditions $u(a)=u(b) = 0$.
We assume that $f$, $g$ and $h$ are continuous functions in the domain $x\in [a,b]$
and that $h(x) \ge 0$. Then the differential equation has a unique solution.
We subdivide our interval $x\in [a,b]$ into $n$ subintervals by setting $x_i =a+ ih$, with $i=0,1,\dots,n+1$.
The step size is then given by $h=(b-a)/(n+1)$ with $n\in {\mathbb{N}}$.
For the internal grid points $i=1,2,\dots n$ we replace the differential operators 
with
\[
u^{''}_i \approx  \frac{u_{i+1} -2u_i +u_{i-i}}{h^2}.
\]
for the second derivative while the first derivative is given by 
\[
u^{'}_i \approx  \frac{u_{i+1} -u_{i-i}}{2h}.
\]

We rewrite our original differential equation in terms of a discretized equation as
\[
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}+g_i\frac{u_{i+1} -u_{i-i}}{2h}+h_iu_i=f_i,
\]
with $i=1,2,\dots, n$. We need to add to this system the two boundary conditions $u(a) =u_0$ and $u(b) = u_{n+1}$.
This equation can again be rewritten as a tridiagonal matrix problem. We leave it as an exercise to the reader 
to find the matrix elements, find the conditions for having weakly dominant diagonal elements and that the matrix is 
irreducible.

%\section{Singular value decomposition}

\section{Spline Interpolation}


Cubic spline interpolation is among one of the most used 
methods for interpolating between data points where the arguments
are organized as ascending series. In the library program we supply
such a function, based on the so-called cubic spline method to be 
described below.  The linear equation solver we developed in the previous section for 
tridiagonal matrices can be reused for spline interpolation.

A spline function consists of polynomial pieces defined on
subintervals. The different subintervals are connected via
various continuity relations.

Assume we have at our disposal $n+1$ points $x_0, x_1, \dots x_n$ 
arranged so that $x_0<x_1<x_2< \dots x_{n-1}<x_n$ (such points are called
knots). A spline function $s$ of degree $k$ with $n+1$ knots is defined
as follows
\begin{itemize}
 \item On every subinterval $[x_{i-1},x_i)$ $s$ is a polynomial
of degree $\le k$.
\item $s$ has $k-1$ continuous derivatives in the whole interval $[x_0,x_n]$.
\end{itemize} 

As an example, consider a spline function of degree $k=1$ defined as follows
\begin{equation}
    s(x)=\left\{\begin{array}{cc} s_0(x)=a_0x+b_0 & x\in [x_0, x_1) \\   
                             s_1(x)=a_1x+b_1 & x\in [x_1, x_2) \\   
                             \dots & \dots \\
                             s_{n-1}(x)=a_{n-1}x+b_{n-1} & x\in 
                             [x_{n-1}, x_n] \end{array}\right.
\end{equation}

In this case the polynomial consists of series of straight lines 
connected to each other at every endpoint. The number of continuous
derivatives is then $k-1=0$, as expected when we deal with straight lines.
Such a polynomial is quite easy to construct given
$n+1$ points $x_0, x_1, \dots x_n$ and their corresponding 
function values. 

The most commonly used spline function is the one with $k=3$, the so-called
cubic spline function. 
Assume that we have in addition to the $n+1$ knots a series of
functions values $y_0=f(x_0), y_1=f(x_1), \dots y_n=f(x_n)$.
By definition, the polynomials $s_{i-1}$ and $s_i$ 
are thence supposed to interpolate
the same point $i$, i.e.,
\be
    s_{i-1}(x_i)= y_i = s_i(x_i),
\ee
with $1 \le i \le n-1$. In total we have $n$ polynomials of the 
type
\be
    s_i(x)=a_{i0}+a_{i1}x+a_{i2}x^2+a_{i3}x^3,
\ee
yielding $4n$ coefficients to determine. Every subinterval provides
in addition two conditions 
\be
    y_i = s(x_i),
\ee
and 
\be
    y_{i+1}=s(x_{i+1}),
\ee
to be fulfilled. If we also assume that $s'$ and $s''$ are continuous,
then
\be
       s'_{i-1}(x_i)= s'_i(x_i),
\ee
yields $n-1$ conditions. Similarly,
\be
       s''_{i-1}(x_i)= s''_i(x_i),
\ee
results in additional $n-1$ conditions. In total we have $4n$ coefficients
and $4n-2$ equations to determine them, leaving us with $2$ degrees of 
freedom to be determined. 

Using the last equation we define two values for the second derivative,
namely
\be
       s''_{i}(x_i)= f_i,
\ee
and 
\be
       s''_{i}(x_{i+1})= f_{i+1},
\ee
and setting up a straight line between $f_i$ and $f_{i+1}$ we have
\be
   s_i''(x) = \frac{f_i}{x_{i+1}-x_i}(x_{i+1}-x)+
               \frac{f_{i+1}}{x_{i+1}-x_i}(x-x_i),
\ee
and integrating twice one obtains
\be
   s_i(x) = \frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3
             +c(x-x_i)+d(x_{i+1}-x).
\ee
Using the conditions $s_i(x_i)=y_i$ and $s_i(x_{i+1})=y_{i+1}$ 
we can in turn determine the constants $c$ and $d$ resulting in
\begin{eqnarray}
   s_i(x) =&\frac{f_i}{6(x_{i+1}-x_i)}(x_{i+1}-x)^3+
               \frac{f_{i+1}}{6(x_{i+1}-x_i)}(x-x_i)^3 \nonumber  \\ 
            +&(\frac{y_{i+1}}{x_{i+1}-x_i}-\frac{f_{i+1}(x_{i+1}-x_i)}{6})
              (x-x_i)+
             (\frac{y_{i}}{x_{i+1}-x_i}-\frac{f_{i}(x_{i+1}-x_i)}{6})
             (x_{i+1}-x).
\end{eqnarray}

How to determine the values of the second
derivatives $f_{i}$ and $f_{i+1}$? We use the continuity assumption 
of the first derivatives 
\be
    s'_{i-1}(x_i)= s'_i(x_i),
\ee
and set $x=x_i$. Defining $h_i=x_{i+1}-x_i$ we obtain finally
the following expression
\be
   h_{i-1}f_{i-1}+2(h_{i}+h_{i-1})f_i+h_if_{i+1}=
   \frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1}),
\ee
and introducing the shorthands $u_i=2(h_{i}+h_{i-1})$, 
$v_i=\frac{6}{h_i}(y_{i+1}-y_i)-\frac{6}{h_{i-1}}(y_{i}-y_{i-1})$,
we can reformulate the problem as a set of linear equations to be 
solved  through e.g., Gaussian elemination, namely
\be
   \left[\begin{array}{cccccccc} u_1 & h_1 &0 &\dots & & & & \\
                                 h_1 & u_2 & h_2 &0 &\dots & & & \\
                                  0   & h_2 & u_3 & h_3 &0 &\dots & & \\
                               \dots& & \dots &\dots &\dots &\dots &\dots & \\
                                 &\dots & & &0 &h_{n-3} &u_{n-2} &h_{n-2} \\
                                 & && & &0 &h_{n-2} &u_{n-1} \end{array}\right]
   \left[\begin{array}{c} f_1 \\ 
                          f_2 \\
                          f_3\\
                          \dots \\
                          f_{n-2} \\ 
                          f_{n-1} \end{array} \right] =
   \left[\begin{array}{c} v_1 \\ 
                          v_2 \\
                          v_3\\
                          \dots \\
                          v_{n-2}\\
                          v_{n-1} \end{array} \right].
\ee
Note that this is a set of tridiagonal equations and can be solved 
through only $O(n)$ operations.

It is easy to write your own program for the cubic spline method when you have written a slover for tridiagonal equations. We split the program into two tasks,
one which finds the polynomial approximation and one which uses the polynomials approximation
to find an interpolated value for a function. These functions are included in the programs of
this chapter, see the codes cubicpsline.cpp and cubicsinterpol.cpp. 
Alternatively, you can solve exercise 6.4!


\section{Iterative Methods}
Till now we have dealt with so-called direct solvers such as Gaussian elimination and LU 
decomposition.  Iterative solvers offer another strategy and are much used in partial
differential equations. We start with a guess for the solution and then iterate till the solution
does not change anymore.
\subsection{Jacobi's method}
It is a simple method for solving
\[ \hat{A}{\bf x}={\bf b},\]
where $\hat{A}$ is a matrix and ${\bf x}$ and ${\bf b}$ are vectors. The vector ${\bf x}$ is 
the unknown.

It is an iterative scheme where we start with a guess for the unknown, and 
after $k+1$ iterations we have  
\[ {\bf x}^{(k+1)}= \hat{D}^{-1}({\bf b}-(\hat{L}+\hat{U}){\bf x}^{(k)}),\]
with $\hat{A}=\hat{D}+\hat{U}+\hat{L}$ and
$\hat{D}$ being a diagonal matrix, $\hat{U}$ an upper triangular matrix and $\hat{L}$ a  lower triangular
matrix.

If the matrix $\hat{A}$ is positive definite or diagonally dominant, one can show that this method will always converge to the exact solution. 

We can demonstrate Jacobi's method by a $4\times 4$ matrix problem. We assume a guess
for the initial vector elements, labeled $x_i^{(0)}$. This  guess represents our first iteration. The new
values are obtained by substitution
\begin{eqnarray}
 x_1^{(1)} =&(b_1-a_{12}x_2^{(0)} -a_{13}x_3^{(0)} - a_{14}x_4^{(0)})/a_{11} \nonumber \\
 x_2^{(1)} =&(b_2-a_{21}x_1^{(0)} - a_{23}x_3^{(0)} - a_{24}x_4^{(0)})/a_{22} \nonumber \\
 x_3^{(1)} =&(b_3- a_{31}x_1^{(0)} -a_{32}x_2^{(0)} -a_{34}x_4^{(0)})/a_{33} \nonumber \\
 x_4^{(1)}=&(b_4-a_{41}x_1^{(0)} -a_{42}x_2^{(0)} - a_{43}x_3^{(0)})/a_{44},  \nonumber
\end{eqnarray}
which after $k+1$ iterations result in 
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \nonumber
\end{eqnarray}

We can generalize the above equations to
\[
 x_i^{(k+1)}=(b_i-\sum_{j=1, j\ne i}^{n}a_{ij}x_j^{(k)})/a_{ii}
\]
or in an even more compact form as
\[ {\bf x}^{(k+1)}= \hat{D}^{-1}({\bf b}-(\hat{L}+\hat{U}){\bf x}^{(k)}),\]
with $\hat{A}=\hat{D}+\hat{U}+\hat{L}$ and
$\hat{D}$ being a diagonal matrix, $\hat{U}$ an upper triangular matrix and $\hat{L}$ a  lower triangular
matrix.
\subsection{Gauss-Seidel}
Our $4\times 4$ matrix problem 
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k)} -a_{32}x_2^{(k)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k)} -a_{42}x_2^{(k)} - a_{43}x_3^{(k)})/a_{44},  \nonumber
\end{eqnarray}
can be rewritten as 
\begin{eqnarray}
 x_1^{(k+1)} =&(b_1-a_{12}x_2^{(k)} -a_{13}x_3^{(k)} - a_{14}x_4^{(k)})/a_{11} \nonumber \\
 x_2^{(k+1)} =&(b_2-a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - a_{24}x_4^{(k)})/a_{22} \nonumber \\
 x_3^{(k+1)} =&(b_3- a_{31}x_1^{(k+1)} -a_{32}x_2^{(k+1)} -a_{34}x_4^{(k)})/a_{33} \nonumber \\
 x_4^{(k+1)}=&(b_4-a_{41}x_1^{(k+1)} -a_{42}x_2^{(k+1)} - a_{43}x_3^{(k+1)})/a_{44},  \nonumber
\end{eqnarray}
which
allows us to utilize the preceding solution (forward substitution). This improves normally the convergence
behavior and leads to the Gauss-Seidel method!

We can generalize these equations to the following form
\[
 x^{(k+1)}_i = \frac{1}{a_{ii}} \left(b_i - \sum_{j>i}a_{ij}x^{(k)}_j - \sum_{j<i}a_{ij}x^{(k+1)}_j \right),\quad i=1,2,\ldots,n. 
\]
The procedure is generally continued until the changes made by an iteration are below some tolerance.

The convergence properties of the Jacobi method and the 
Gauss-Seidel method depend on the matrix $\hat{A}$. These methods converge when
the matrix is symmetric positive-definite, or is strictly or irreducibly diagonally dominant.
Both methods sometimes converge even if these conditions are not satisfied.
\subsection{Successive over-relaxation}
We can rewrite the above in a slightly more formal way and extend the methods to what is 
called successive over-relaxation.
Given a square system of n linear equations with unknown $\mathbf x$:
\[
    \hat{A}\mathbf x = \mathbf b
\]
where:
\[
    \hat{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\a_{n1} & a_{n2} & \cdots & a_{nn} \end{bmatrix}, \qquad \mathbf{x} = \begin{bmatrix} x_{1} \\ x_2 \\ \vdots \\ x_n \end{bmatrix} , \qquad \mathbf{b} = \begin{bmatrix} b_{1} \\ b_2 \\ \vdots \\ b_n \end{bmatrix}.
\]
Then A can be decomposed into a diagonal component D, and strictly lower and upper triangular components L and U:
\[
    \hat{A} =\hat{D} + \hat{L} + \hat{U},
\]
where
\[
    D = \begin{bmatrix} a_{11} & 0 & \cdots & 0 \\ 0 & a_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & a_{nn} \end{bmatrix}, \quad L = \begin{bmatrix} 0 & 0 & \cdots & 0 \\ a_{21} & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\a_{n1} & a_{n2} & \cdots & 0 \end{bmatrix}, \quad U = \begin{bmatrix} 0 & a_{12} & \cdots & a_{1n} \\ 0 & 0 & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & 0 \end{bmatrix}. 
\]
The system of linear equations may be rewritten as:
\[
    (D+\omega L) \mathbf{x} = \omega \mathbf{b} - [\omega U + (\omega-1) D ] \mathbf{x} 
\]
for a constant $\omega > 1$.
The method of successive over-relaxation is an iterative technique that solves the left hand side of this expression for $x$, using previous value for $x$ on the right hand side. Analytically, this may be written as:
\[
    \mathbf{x}^{(k+1)} = (D+\omega L)^{-1} \big(\omega \mathbf{b} - [\omega U + (\omega-1) D ] \mathbf{x}^{(k)}\big). 
\]
However, by taking advantage of the triangular form of $(D+\omega L)$, the elements of $x^{(k+1)}$ can be computed sequentially using forward substitution:
\[
    x^{(k+1)}_i = (1-\omega)x^{(k)}_i + \frac{\omega}{a_{ii}} \left(b_i - \sum_{j>i} a_{ij}x^{(k)}_j - \sum_{j<i} a_{ij}x^{(k+1)}_j \right),\quad i=1,2,\ldots,n. 
\]
The choice of relaxation factor is not necessarily easy, and depends upon the properties of the coefficient matrix. For symmetric, positive-definite matrices it can be proven that $0 < \omega < 2$ will lead to convergence, but we are generally interested in faster convergence rather than just convergence.

%\subsection{Parallel Jacobi Algorithm}

% add about parallelization
\subsection{Conjugate Gradient Method}
% add more text and examples of code here
% add about parallel CG
The success of the Conjugate Gradient 
method  for finding solutions of non-linear problems is based
on the theory for of conjugate gradients for linear systems of equations. It belongs
to the class of iterative methods for solving problems from linear algebra of the type
\[
  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}.
\]
In the iterative process we end up with a problem like
\[
  \hat{\bf {r}}= \hat{\bf {b}}-\hat{{\bf A}}\hat{\bf {x}},
\]
where $\hat{\bf {r}}$ is the so-called residual or error in the iterative process.

The residual is zero when we reach the minimum of the quadratic equation
\[
  P(\hat{\bf {x}})=\frac{1}{2}\hat{\bf {x}}^T\hat{{\bf A}}\hat{\bf {x}} - \hat{\bf {x}}^T\hat{\bf {b}},
\]
with the constraint that the matrix $\hat{{\bf A}}$ is positive definite and symmetric.
If we search for a minimum of the quantum mechanical  variance, then the matrix 
$\hat{{\bf A}}$, which is called the Hessian, is given by the second-derivative of the variance.  This quantity is always positive definite. If we vary the energy, the Hessian may not always be positive definite. 

In the Conjugate Gradient method we define so-called conjugate directions and two vectors 
$\hat{\bf {s}}$ and $\hat{\bf {t}}$
are said to be
conjugate if 
\[
\hat{\bf {s}}^T\hat{{\bf A}}\hat{\bf {t}}= 0.
\]
The philosophy of the Conjugate Gradient method is to perform searches in various conjugate directions
of our vectors $\hat{{\bf x}}_i$ obeying the above criterion, namely
\[
\hat{\bf {x}}_i^T\hat{{\bf A}}\hat{\bf {x}}_j= 0.
\]
Two vectors are conjugate if they are orthogonal with respect to 
this inner product. Being conjugate is a symmetric relation: if $\hat{\bf {s}}$ is conjugate to $\hat{\bf {t}}$, then $\hat{\bf {t}}$ is conjugate to $\hat{\bf {s}}$.

An example is given by the eigenvectors of the matrix 
\[
\hat{\bf {v}}_i^T\hat{{\bf A}}\hat{\bf {v}}_j= \lambda\hat{\bf {v}}_i^T\hat{\bf {v}}_j,
\]
which is zero unless $i=j$. 

Assume now that we have a symmetric positive-definite matrix $\hat{\bf {A}}$ of size
$n\times n$. At each iteration $i+1$ we obtain the conjugate direction of a vector 
\[
\hat{\bf {x}}_{i+1}=\hat{\bf {x}}_{i}+\alpha_i\hat{\bf {p}}_{i}. 
\]
We assume that $\hat{\bf {p}}_{i}$ is a sequence of $n$ mutually conjugate directions. 
Then the $\hat{\bf {p}}_{i}$  form a basis of $R^n$ and we can expand the solution 
$  \hat{{\bf A}}\hat{\bf {x}} = \hat{\bf {b}}$ in this basis, namely
\[
  \hat{\bf {x}}  = \sum^{n}_{i=1} \alpha_i \hat{\bf {p}}_i.
\]

The coefficients are given by
\[
    \mathbf{A}\mathbf{x} = \sum^{n}_{i=1} \alpha_i \mathbf{A} \mathbf{p}_i = \mathbf{b}.
\]
Multiplying with $\hat{\bf {p}}_k^T$  from the left gives
\[
  \hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {x}} = \sum^{n}_{i=1} \alpha_i\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {p}}_i= \hat{\bf {p}}_k^T \hat{\bf {b}},
\]
and we can define the coefficients $\alpha_k$ as 
\[
    \alpha_k = \frac{\hat{\bf {p}}_k^T \hat{\bf {b}}}{\hat{\bf {p}}_k^T \hat{\bf {A}} \hat{\bf {p}}_k}
\] 

If we choose the conjugate vectors $\hat{\bf {p}}_k$ carefully, 
then we may not need all of them to obtain a good approximation to the solution 
$\hat{\bf {x}}$. 
So, we want to regard the conjugate gradient method as an iterative method. 
This also allows us to solve systems where $n$ is so large that the direct 
method would take too much time.

We denote the initial guess for $\hat{\bf {x}}$ as $\hat{\bf {x}}_0$. 
We can assume without loss of generality that 
\[
\hat{\bf {x}}_0=0,
\]
or consider the system 
\[
\hat{\bf {A}}\hat{\bf {z}} = \hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_0,
\]
instead.

One can show that the solution $\hat{\bf {x}}$ is also the unique minimizer of the quadratic form
\[
  f(\hat{\bf {x}}) = \frac{1}{2}\hat{\bf {x}}^T\hat{\bf {A}}\hat{\bf {x}} - \hat{\bf {x}}^T \hat{\bf {x}} , \quad \hat{\bf {x}}\in\mathbf{R}^n. 
\]
This suggests taking the first basis vector $\hat{\bf {p}}_1$ 
to be the gradient of $f$ at $\hat{\bf {x}}=\hat{\bf {x}}_0$, 
which equals 
\[
\hat{\bf {A}}\hat{\bf {x}}_0-\hat{\bf {b}},
\]
and 
$\hat{\bf {x}}_0=0$ it is equal $-\hat{\bf {b}}$.
The other vectors in the basis will be conjugate to the gradient, 
hence the name conjugate gradient method.

Let  $\hat{\bf {r}}_k$ be the residual at the $k$-th step:
\[
\hat{\bf {r}}_k=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k.
\]

Note that $\hat{\bf {r}}_k$ is the negative gradient of $f$ at 
$\hat{\bf {x}}=\hat{\bf {x}}_k$, 
so the gradient descent method would be to move in the direction $\hat{\bf {r}}_k$. 
Here, we insist that the directions $\hat{\bf {p}}_k$ are conjugate to each other, 
so we take the direction closest to the gradient $\hat{\bf {r}}_k$  
under the conjugacy constraint. 
This gives the following expression
\[
\hat{\bf {p}}_{k+1}=\hat{\bf {r}}_k-\frac{\hat{\bf {p}}_k^T \hat{\bf {A}}\hat{\bf {r}}_k}{\hat{\bf {p}}_k^T\hat{\bf {A}}\hat{\bf {p}}_k} \hat{\bf {p}}_k.
\]

We can also  compute the residual iteratively as
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_{k+1},
 \]
which equals
\[
\hat{\bf {b}}-\hat{\bf {A}}(\hat{\bf {x}}_k+\alpha_k\hat{\bf {p}}_k),
 \]
or
\[
(\hat{\bf {b}}-\hat{\bf {A}}\hat{\bf {x}}_k)-\alpha_k\hat{\bf {A}}\hat{\bf {p}}_k,
 \]
which gives
\[
\hat{\bf {r}}_{k+1}=\hat{\bf {r}}_k-\hat{\bf {A}}\hat{\bf {p}}_{k},
 \]

If we consider finding the minimum of a function $f$ using Newton's method,
that implies a  search for a zero of the gradient of a function.  Near a point $x_i$
we have to second order
\[
f(\hat{\bf {x}})=f(\hat{\bf {x}}_i)+(\hat{\bf {x}}-\hat{\bf {x}}_i)\nabla f(\hat{\bf {x}}_i)
\frac{1}{2}(\hat{\bf {x}}-\hat{\bf {x}}_i)\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i)
\]
giving
\[
\nabla f(\hat{\bf {x}})=\nabla f(\hat{\bf {x}}_i)+\hat{\bf {A}}(\hat{\bf {x}}-\hat{\bf {x}}_i).
 \]
In Newton's method we set $\nabla f = 0$ and we can thus compute the next iteration point
\[
\hat{\bf {x}}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}\nabla f(\hat{\bf {x}}_i).
\]
Subtracting this equation from that of $\hat{\bf {x}}_{i+1}$ we have
\[
\hat{\bf {x}}_{i+1}-\hat{\bf {x}}_i=\hat{\bf {A}}^{-1}(\nabla f(\hat{\bf {x}}_{i+1})-\nabla f(\hat{\bf {x}}_i)).
\]

%\section{Singular value decomposition}

\section{A vector and matrix class}
We end this chapter by presenting a class which allows to manipulate one- and two-dimensional arrays. 
However, before we proceed, we would like to come with some general recommendations. Although it is useful to write your own
classes, like the one included here, in general these classes may not be very efficient from a computational point of view.
There are several libraries which include many interesting array features that allow us to write more compact code. The latter
has the advantage that the code is lost likely easier to debug in case of errors (obviously assuming that the library is functioning
correctly). Furthermore, if the proper functionalities  are included, the final code may closely resemble 
the mathematical operations we wish to perform, increasing considerably the readability of our program. And finally, the code is in almost all casesmuch faster than the one we wrote!

In particular, we would like to recommend the C++ linear algebra library Armadillo, see \url{http://arma.sourceforgenet}.
For those of you who are familiar with compiled programs like Matlab, the syntax is deliberately similar.
Integer, floating point and complex numbers are supported, as well as a subset of trigonometric and statistics functions. 
Various matrix decompositions are provided through optional integration with LAPACK, or one of its high performance drop-in replacements (such as the multi-threaded MKL or ACML libraries).
The selected examples included here show some examples on how to declare arrays and rearrange arrays or perform mathematical operations
on say vectors or matrices. The first example here defines two random matrices 
of dimensionality $10\times 10$ and performs a matrix-matrix multiplication using the $dgemm$ function of the library BLAS. 
\begin{lstlisting}[title={Simple matrix-matrix multiplication of two random matrices}]
#include <iostream>
#include <armadillo>

using namespace std;
using namespace arma;

int main(int argc, char** argv)
  {
  mat A = randu<mat>(10,10);
  mat B = randu<mat>(10,10);
  //  Matrix-matrix multiplication
  cout << A*B << endl;
  return 0;
  }
\end{lstlisting}
In the next example  we compute the determinant of a $5\times 5$ matrix, its inverse  
and perform thereafter several operations  on various matrices.
\begin{lstlisting}[title={Determinant and inverse of a matrix}]
#include <iostream>
#include "armadillo"
using namespace arma;
using namespace std;

int main(int argc, char** argv)
  {
  cout << "Armadillo version: " << arma_version::as_string() << endl;
  mat A;
  // Hard coding of the matrix
  // endr indicates "end of row"
  A << 0.165300 << 0.454037 << 0.995795 << 0.124098 << 0.047084 << endr
    << 0.688782 << 0.036549 << 0.552848 << 0.937664 << 0.866401 << endr
    << 0.348740 << 0.479388 << 0.506228 << 0.145673 << 0.491547 << endr
    << 0.148678 << 0.682258 << 0.571154 << 0.874724 << 0.444632 << endr
    << 0.245726 << 0.595218 << 0.409327 << 0.367827 << 0.385736 << endr;
  // .n_rows = number of rows
  // .n_cols = number of columns
  cout << "A.n_rows = " << A.n_rows << endl;
  cout << "A.n_cols = " << A.n_cols << endl;
  // Print the matrix A  
  A.print("A =");
  // Computation of the determinant
  cout << "det(A) = " << det(A) << endl;
  // inverse
  cout << "inv(A) = " << endl << inv(A) << endl;
  // save to disk
  A.save("MatrixA.txt", raw_ascii);
  // Define a new matrix B which reads A from file
  mat B;
  B.load("MatrixA.txt");
  B += 5.0*A;
  B.print("The matrix B:");
  // generate the identity matrix
  mat C = eye<mat>(4,4);
  // transpose of B
  cout << "trans(B) =" << endl;
  // maximum from each column (traverse along rows)
  cout << "max(B) =" << endl;
  cout << max(B) << endl;
  // sum of all elements B
  cout << "sum(sum(B)) = " << sum(sum(B)) << endl;
  cout << "accu(B)     = " << accu(B) << endl;
  // trace = sum along diagonal
  cout << "trace(B)    = " << trace(B) << endl;
  // random matrix -- values are uniformly distributed in the [0,1] interval
  mat D = randu<mat>(4,4);
  D.print("Matrix D:");
  // sum of four matrices (no temporary matrices are created)
  mat E = A+B + C + D;
  F.print("F:");
  return 0;
}
\end{lstlisting}
For more examples, please consult the online manual, see \url{http://arma.sourceforgenet}.
\subsection{How to construct your own matrix-vector class}
The rest of this section shows how one can build a matrix-vector class.
We first give an example of a function which use the header file \lstinline{Array.h}. 
\begin{lstlisting}
#include "Array.h"

#include <iostream>
using namespace std;

int main(){

  // Create an array with (default) nrows = 1, ncols = 1:
  Array<double> v1;

  // Redimension the array to have length n:
  int n1 = 3;
  v1.redim(n1);

  // Extract the length of the array:
  const int length = v1.getLength();

  // Create a narray of specific length:
  int n2 = 5;
  Array<double> v2(n2);

  // Create an array as a copy of another one:
  Array<double> v5(v1);

  // Assign the entries in an array:
  v5(0) = 3.0;  
  v5(1) = 2.5;  
  v5(2) = 1.0;  

  for(int i=0; i<3; i++){
	  cout << v5(i) << endl;
  }
  
  // Extract the ith component of an array:
  int i = 2;
  double value = v5(1);
  cout << "value: " << value << endl;

  // Set an array equal another one:
  Array<double> v6 = v5;

  for(int i=0; i<3; i++){
	  v1(i) = 1.0;
	  v2(i) = 2.0;
  }
  
  // Create a two-dimensional array (matrix):
  Array<double> matrix(2, 2);
  
  // Fill the array:
  matrix(0,0) = 1;
  matrix(0,1) = 2;
  matrix(1,0) = 3;
  matrix(1,1) = 4;
  
  // Get the entries in the array:
  cout << "\nMatrix: " << endl;
  for(int i=0; i<2; i++){
    for(int j=0; j<2; j++){
      cout << matrix(i,j) << "   ";
    }
    cout << endl;
  }
  
  // Assign an entry of the matrix to a variable:
  double scalar = matrix(0,0);
  const double b = matrix(1,1);
   
    
  Array<double> vector(2);
  vector(0) = 1.0; 
  vector(1) = 2.0;
    
  Array<double> v = vector;
  Array<double> A = matrix;
  Array<double> u(2);
  
  cout << "\nMatrix: " << endl;
  for(int i=0; i<2; i++){
    for(int j=0; j<2; j++){
      cout << matrix(i,j) << "   ";
    }
    cout << endl;
  }
  
  Array<double> a(2,2);
  a(1,1) = 5.0;
  
  // Arithmetic operations with arrays using a 
  // syntax close to the mathematical language
  Array<double> w = v1 + 2.0*v2;
  
	// Create multidimensional matrices and assign values to them:
  int N = 3;
  Array<double> multiD;  multiD.redim(N,N,N);
  for(int i=0; i<N; i++){
	  for(int j=0; j<N; j++){
		  for(int k=0; k<N; k++){
			  cout << "multD(i,j,k) = " << multiD(i,j,k) << endl;
		  }
	  }
  }

  multiD(1,2,3) = 4.0;
  cout << "multiD(1,2,3) = " << multiD(1,2,3) << endl;
}
\end{lstlisting}
The header file follows here
\begin{lstlisting}
#ifndef ARRAY_H
#define ARRAY_H

#include <iostream>
#include <sstream>
#include <iomanip>
#include <cstdlib>

using namespace std;

template<class T>
class Array{
  private:
    static const int MAXDIM = 6;
		T *data ;    				/**> One-dimensional array of data.*/
  	int size[MAXDIM];		/**> Size of each dimension.*/
		int ndim;						/**> Number of dimensions occupied. */
		int length;					/**> Total number of entries.*/
  
		int dx1, dx2, dx3, dx4, dx5;	
  
		void allocate(int ni=0, int nj=0, int nk=0, int nl=0, int nm=0, int nn=0){
			ndim = MAXDIM;
			
			// Set the number of entries in each dimension.
			size[0]=ni;
			size[1]=nj;
			size[2]=nk;
			size[3]=nl;
			size[4]=nm;
			size[5]=nn;			
			
			
			// Set the number of dimensions used.
			if(size[5] == 0)
				ndim--;
			if(size[4] == 0)
				ndim--;
			if(size[3] == 0)
				ndim--;
			if(size[2] == 0)
				ndim--;
			if(size[1] == 0) 
				ndim--;
			if(size[0] == 0){
				ndim 		= 0;
				length 	= 0;
				data   	= NULL;
			}else{
				try{
					int i;
							
					// Set the length (total number of entries) of the one-dimensional array.
					length = 1;
					for(i=0; i<ndim; i++)
						length *= size[i];
						
						data = new T[length];
					
						dx1 = 		size[0];
						dx2 = dx1*size[1];
						dx3 = dx2*size[2];
						dx4 = dx3*size[3];
						dx5 = dx4*size[4];
											
				}catch(std::bad_alloc&){
					std::cerr << "Array::allocate -- unable to allocate array of length " << length << std::endl;
					exit(1);
				}
			}
			
		}
    
  public:
          
		
    /**
		* @brief Constructor with default arguments. 
		*
		* Creates an array with one or two-dimensions.
		*
		* @param int nrows. Number of rows in the array.
		* @param int ncolsd. Number of columns in the array.
		**/
    Array(int ni=0, int nj=0, int nk=0, int nl=0, int nm=0, int nn=0){
			// Allocate memory
			allocate(ni,nj,nk,nl,nm,nn);
		} // end constructor
			
			
		//! Constructor	
		Array(T* array, int ndim_, int size_[]){
			ndim = ndim_;
			
		  length = 1;
			int i;
			for(i=0; i<ndim; i++){
				size[i] = size_[i];	// Copy only the ndim entries. The rest is zero by default.
				length *= size[i];
			}
							
			// Now when we known the length, we should not forget to allocate memory!!!!
			data = new T[length];
			
			// Copy the entries from array to data:
			for(i=0; i<length; i++){
			  data[i] = array[i];
			}
			
		} // End constructor.
		
		
			
		
		//! Copy constructor
    Array(const Array<T>& array);    
    
    //! Destructor
    ~Array();
				
		
		/**
		* @brief Checks the validity of the indexing.
		* @param i, an integer for indexing the rows.
		* @param j, an integer for indexing the columns.
		**/
		bool indexOk(int i, int j=0) const;    
    
    /**
		* @brief Change the dimensions of an array.
		* @param ni number of entries in the first dimension.
		* @param nj number of entries in the second dimension.
		* @param nk number of entries in the third dimension.
		*	@param nl number of entries in the fourth dimension.
		* @param nm number of entries in the fifth dimension.
		* @param nn number of entries in the sixth dimension.
		**/
    bool redim(int ni, int nj=0, int nk=0, int nl=0, int nm=0, int nn=0);		
		
		/**
		* @return The total number of entries in the array, i.e., the sum of the entries in all the dimensions.
		**/
		int getLength()const{return length;}
		
		
		/**
		* @return The number of rows in a matrix.
		**/
		int getRows() const {return size[0];}
		
		/**
		* @return Returns the number of columns in a matrix.
		**/
    int getColumns() const {return size[1];}
    
    
    /** @brief Gives the number of entries in a dimension.
		*
		*	@param i An integer from 0 to 5 indicating the dimension we want to explore. 
		*	@return size[i] An integer for the number of elements in the dimension number i.
		**/		
		int dimension(int i) const{return size[i];}
		
		
		/**
		* The number of dimensions in the array.
		**/
		int getNDIM()const{return ndim;} 
    
    
    /** 
		* @return A constant pointer to the array of data.
		* This function can be used to interface C++ with Python/Fortran/C.
		**/
		const T* getPtr() const;
    
		
		/**
		* @return A pointer to the array of data.
		* This function can be used to interface C++ with Python/Fortran/C.
		**/
		T* getPtr();
		
		
		/**
		*	@return A pointer to an array with information on the length of each dimension. 
		**/
		int* getPtrSize();
		
		
    	
    
		/************************************************************/
		/*											OPERATORS														*/
		/************************************************************/
		
    //! Assignment operator
    Array<T>& operator=(const Array<T>& array);

    //! Sum operator
    Array<T> operator+(const Array<T>& array);
      
    //! Substraction operator    
		Array<T> operator-(const Array<T>& array)const; /// w=u-v;
		
		
		//! Multiplication operator
    //Array<T> operator*(const Array<T>& array);

		
		//! Assigment by addition operator
		Array<T>& operator+=(const Array<T>& w);


		//! Assignment by substraction operator
    Array<T>& operator-=(const Array<T>& w);
    
    
    //! Assignment by scalar product operator
    Array<T>& operator*=(double scalar);
		
    //! Assignment by division operator
		Array<T>& operator/=(double scalar);	
		
    //! Index operators
    const T& operator()(int i)const;	
    const T& operator()(int i, int j)const;	
		const T& operator()(int i, int j, int k)const;	
		const T& operator()(int i, int j, int k, int l)const;		
		const T& operator()(int i, int j, int k, int l, int m)const;		
		const T& operator()(int i, int j, int k, int l, int m, int n)const;		
		
    T& operator()(int i);
		T& operator()(int i, int j);
		T& operator()(int i, int j, int k);	
		T& operator()(int i, int j, int k, int l);		
		T& operator()(int i, int j, int k, int l, int m);		
		T& operator()(int i, int j, int k, int l, int m, int n);		
		
		
   		
		/**************************************************************/
		/*								FRIEND FUNCTIONS 														*/
		/**************************************************************/
 		//! Unary operator +
		template <class T2>
    friend Array<T> operator+ (const Array<T>&);                 // u = + v
 
		//! Unary operator -
		template <class T2>
    friend Array<T> operator-(const Array<T>&);                 // u = - v
    
		
			
		
		/**
    * Premultiplication by a floating point number: 
    * \f$\mathbf{u} = a \mathbf{v}\f$, 
    * where \f$a\f$ is a scalar and \f$\mathbf{v}\f$ is a array.
    **/
		template <class T2>
    friend Array<T> operator*(double, const Array<T>&);         // u = a*v
    
    /**
    * Postmultiplication by a floating point number: 
    * \f$\mathbf{u} = \mathbf{v} a\f$, 
    * where \f$a\f$ is a scalar and \f$\mathbf{v}\f$ is a array.
    **/
		template <class T2>
    friend Array<T> operator*(const Array<T>&, double);         // u = v*a
  
		
		/**
    * Division of the entries of a array by a scalar.
    **/
		template <class T2>
    friend Array<T> operator/(const Array<T>&, double);         // u = v/a 
		
		
		
		
		
};

#include "Array.cpp"


// Destructor
template <class T>
inline Array<T>::~Array(){delete[] data;}

// Index operators
template <class T>
inline const T& Array<T>::operator()(int i)const {
	#if CHECKBOUNDS_ON
		indexOk(i);
	#endif
	return data[i];
}



template <class T>
inline const T& Array<T>::operator()(int i, int j)const {
	#if CHECKBOUNDS_ON
		indexOk(i,j);
	#endif
	
	return data[i + j*dx1];
}

template <class T>
inline const T& Array<T>::operator()(int i, int j, int k)const {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k);
	#endif
	
	return data[i + j*dx1 + k*dx2];
}


template <class T>
inline const T& Array<T>::operator()(int i, int j, int k, int l)const {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k,l);
	#endif
	
	return data[i + j*dx1 + k*dx2 + l*dx3];
}


template <class T>
inline const T& Array<T>::operator()(int i, int j, int k, int l, int m)const {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k,l, m);
	#endif
	
	return data[i + j*dx1 + k*dx2 + l*dx3 + m*dx4];
}


template <class T>
inline const T& Array<T>::operator()(int i, int j, int k, int l, int m, int n)const {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k,l,m,n);
	#endif
	
	return data[i + j*dx1 + k*dx2 + l*dx3 + m*dx4 + n*dx5];
}
	
template <class T>
inline T& Array<T>::operator()(int i) {
	#if CHECKBOUNDS_ON
		indexOk(i);
	#endif
	return data[i];
}
	

template <class T>
inline T& Array<T>::operator()(int i, int j) {
	#if CHECKBOUNDS_ON
		indexOk(i,j);
	#endif
	
	return data[i + j*dx1];
}


template <class T>
inline T& Array<T>::operator()(int i, int j, int k) {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k);
	#endif
	
	return data[i + j*dx1 + k*dx2];
}


template <class T>
inline T& Array<T>::operator()(int i, int j, int k, int l) {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k,l);
	#endif
	
	return data[i + j*dx1 + k*dx2 + l*dx3];
}


template <class T>
inline T& Array<T>::operator()(int i, int j, int k, int l, int m) {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k,l,m);
	#endif
	
	return data[i + j*dx1 + k*dx2 + l*dx3 + m*dx4];
}


template <class T>
inline T& Array<T>::operator()(int i, int j, int k, int l, int m, int n) {
	#if CHECKBOUNDS_ON
		indexOk(i,j,k,l,m,n);
	#endif
	
	return data[i + j*dx1 + k*dx2 + l*dx3 + m*dx4 + n*dx5];
}



template <class T>
inline const T* Array<T>::getPtr() const {return data;}



template <class T>
inline T* Array<T>::getPtr(){return data; }


template <class T>
inline int* Array<T>::getPtrSize(){return size;}


// template <class T>
// inline int Array<T>::dim()const{return ndim;}


/******************************************************************/
/*							IMPLEMENTATION OF FRIEND FUNCTIONS								*/
/******************************************************************/

/******************************************************************/
/*             (Arithmetic) Unary operators                       */
/******************************************************************/
//! Unary operator +
template <class T>
inline Array<T> operator+(const Array<T>& v){     // u = + v
	return v;
}


//! Unary operator -
template <class T>
inline Array<T> operator-(const Array<T>& v){      // u = - v
	return Array<T>(v.size[0],v.size[1]) -v;
}


//! Postmultiplication operator
template <class T>
inline Array<T> operator*(const Array<T>& v, double scalar){   // u = v*a
  return Array<T>(v) *= scalar;
}


//! Premultiplication operator. 
template <class T>
inline Array<T> operator*(double scalar, const Array<T>& v){   // u = a*v
  return v*scalar;  // Note the call to postmultiplication operator defined above
}


//! Division of the entries in a array by a scalar
template <class T>
inline Array<T> operator/(const Array<T>& v, double scalar){ 
  if(!scalar) std::cout << "Division by zero!" << std::endl;
  return (1.0/scalar)*v;
}

#endif 
\end{lstlisting}


\section{Exercises}

%\subsection*{Exercise 6.1: Write your own Gaussian elimination code}
\begin{prob} 
The aim of this exercise is to write your own Gaussian elimination code.
\begin{enumerate}
\item 
Consider the linear system of equations 
%
\begin{eqnarray}
 a_{11}x_1 +a_{12}x_2 +a_{13}x_3 =&w_1 \nonumber \\
a_{21}x_1 + a_{22}x_2 + a_{23}x_3=&w_2 \nonumber \\
a_{31}x_1 + a_{32}x_2 + a_{33}x_3=&w_3. \nonumber 
\end{eqnarray}
This can be written in matrix form as
\[
   {\bf Ax}={\bf w}.
\]

We specialize here to the following case
\begin{eqnarray}
 -x_1 +x_2 -4x_3 =&0 \nonumber \\
  2x_1 + 2x_2 =&1 \nonumber \\
3x_1 + 3x_2 + 2x_3=&\frac{1}{2}. \nonumber 
\end{eqnarray} 
Obtain the solution (by hand) of this system of equations by doing Gaussian elimination.

\item 
Write therafter a program 
which implements Gaussian elimination (with pivoting)  
and solve the above system of linear equations. How many floating point operations are 
involved in the solution via Gaussian elimination without pivoting?
Can you estimate the number of floating point operations with pivoting?
\end{enumerate}
\end{prob}
%\subsection*{Exercise 6.2: Cholesky factorization}
\begin{prob}
 If the matrix $A$ is real, symmetric and positive definite, then
it has  a unique factorization (called Cholesky factorization)
\[
   A = LU = LL^T
\]
where $L^T$ is the upper matrix, implying that
\[
  L^T_{ij} = L_{ji}.
\]
The algorithm for the Cholesky decomposition
is a special case of the general LU-decomposition algorithm.
The algorithm of this decomposition is as follows
\begin{itemize}
\item Calculate the diagonal element $L_{ii}$ by setting up a loop 
for $i=0$ to $i=n-1$ (C++ indexing of matrices and vectors)
\begin{equation}
   L_{ii} = \left(A_{ii} - \sum_{k=0}^{i-1}L_{ik}^2\right)^{1/2}.
\end{equation}
%
\item within the loop over $i$, introduce a new loop which goes 
from $j=i+1$ to $n-1$ and calculate 
%
\begin{equation}
      L_{ji} =
      \frac{1}{L_{ii}}\left(A_{ij}-\sum_{k=0}^{i-1}L_{ik}l_{jk}\right).
\end{equation}
\end{itemize}
For the Cholesky algorithm we have always that $L_{ii} > 0$ and the problem
with exceedingly large matrix elements does not appear and hence there is no
need for pivoting.
Write a function which performs the Cholesky decomposition.
Test your program against the standard LU decomposition by using the matrix
\begin{equation}
 {\bf A} =
      \left( \begin{array}{ccc} 6 & 3 & 2 \\
                                 3 & 2 & 1 \\
                                 2 & 1 & 1 
             \end{array} \right)
\end{equation}

Finally, use the Cholesky method to solve
\begin{eqnarray}
 0.05x_1 +0.07x_2+0.06x_3 +0.05x_4 =&0.23 \nonumber \\
0.07x_1 +0.10x_2 + 0.08x_3 + 0.07x_4=&0.32 \nonumber \\
0.06x_1 + 0.08x_2 + 0.10x_3 + 0.09x_4=&0.33 \nonumber \\
0.05x_1 + 0.07x_2 + 0.09x_3 + 0.10x_4=&0.31 \nonumber
\end{eqnarray}
You can also use the LU codes for linear equations to check the results. 
\end{prob}
%\subsection*{Project 6.1: The one-dimensional Poisson equation}

\begin{prob}
In this exercise we are going to solve the one-dimensional Poisson equation 
in terms of linear equations.
\begin{enumerate}
\item
We are going to solve the one-dimensional Poisson equation
with Dirichlet boundary conditions by rewriting it as a set of linear equations.

The three-dimensional Poisson equation is a partial differential equation, 
\[ 
\frac{\partial^2\phi}{\partial x^2} +\frac{\partial^2\phi}{\partial y^2}+\frac{\partial^2\phi}{\partial z^2} = -\frac{\rho(x,y,z)}{\epsilon_0}, 
\]
whose solution we will discuss in chapter \ref{chap:partial}. The function $\rho(x,y,z)$ is the charge density and $\phi$ is the
electrostatic potential.  In this project we consider the one-dimensional case  since 
there are a few situations, 
possessing a high degree of symmetry, where it is possible to find analytic solutions. Let us discuss some of these solutions.

Suppose, first of all, that there is no variation of the various quantities in 
the $y$- and $z$-directions. In this case, Poisson's equation reduces to an ordinary differential equation in $x$, 
the solution of which is relatively straightforward. 
Consider for example a vacuum diode, in which electrons are emitted from a hot cathode and accelerated towards an anode.
The anode is held at a large positive potential $V_0$ with respect to the cathode. 
We can think of this as an essentially one-dimensional problem. 
Suppose that the cathode is at $x=0$ and the anode at $x=d$. Poisson's equation takes the form
\[ \frac{d^2\phi}{dx^2} = - \frac{\rho(x)}{\epsilon_0},\]
where $\phi(x)$ satisfies the boundary conditions $\phi(0)=0$ and $\phi(d)=V_0$. By energy conservation, an electron emitted from rest 
at the cathode has an $x$-velocity $v(x)$ which satisfies
\[\frac{1}{2} m_e v^2(x) - e \phi(x) = 0. \] 

Furthermore, we assume that the current $I$ is independent of $x$ between the anode and cathode, otherwise, 
charge will build up at some points. From electromagnetism one can then show that the current $I$ is given by
$I = -\rho(x) v(x) A$, 
where $A$ is the cross-sectional area of the diode. The previous equations can be combined to give
\[ \frac{d^2\phi}{dx^2} = \frac{I}{\epsilon_0 A}\left(\frac{m_e}{2 e}\right)^{1/2}\phi^{-1/2}. \] 
The solution of the above equation which satisfies the boundary conditions is
\[ \phi = V_0 \left(\frac{x}{d}\right)^{4/3}, \]
with
\begin{displaymath} I = \frac{4}{9}\frac{\epsilon_0 A}{d^2}\left(\frac{2 e}{m_e}\right)^{1/2} V_0^{3/2}. \end{displaymath} 
This relationship between the current and the voltage in a vacuum diode is called the Child-Langmuir law.


Another physics example in one dimension is the famous Thomas-Fermi model, widely used as a mean-field
model  in simulations of quantum mechanical systems \cite{thomas1927,fermi1927}, see Lieb for a newer and updated discussion \cite{lieb1981}.
Thomas and Fermi assumed the existence of an energy functional, and derived an expression for the kinetic energy based on the density of electrons, 
$\rho(r)$ in an infinite potential well. For a large atom or molecule with a 
large number of electrons. Schrdinger's equation, which would give the exact density and energy, cannot be 
easily handled for large numbers of interacting particles. Since the Poisson equation connects the electrostatic potential with the charge density,
one can derive the following equation for potential $V$ 
\[ \frac{d^2 V}{dx^2} = \frac{V^{3/2}}{\sqrt{x}}, \]
with $V(0)=1$. 


In our case we will rewrite Poisson's equation in terms of dimensionless variables. We can then rewrite the equation as
\[
-u''(x) = f(x), \hspace{0.5cm} x\in(0,1), \hspace{0.5cm} u(0) = u(1) = 0.
\]
and we define the discretized approximation  to $u$ as $v_i$  with 
grid points $x_i=ih$   in the interval from $x_0=0$ to $x_{n+1}=1$.
The step length or spacing is defined as $h=1/(n+1)$. 
We have then the boundary conditions $v_0 = v_{n+1} = 0$.
We  approximate the second
derivative of $u$ with 
\[
   -\frac{v_{i+1}+v_{i-1}-2v_i}{h^2} = f_i  \hspace{0.5cm} \mathrm{for} \hspace{0.1cm} i=1,\dots, n,
\]
where $f_i=f(x_i)$.
Show that you can rewrite this equation as a linear set of equations of the form 
\[
   {\bf A}{\bf v} = \tilde{{\bf b}},
\]
where ${\bf A}$ is an $n\times n$  tridiagonal matrix which we rewrite as 
\[
    {\bf A} = \left(\begin{array}{cccccc}
                           2& -1& 0 &\dots   & \dots &0 \\
                           -1 & 2 & -1 &0 &\dots &\dots \\
                           0&-1 &2 & -1 & 0 & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           0&\dots   &  &-1 &2& -1 \\
                           0&\dots    &  & 0  &-1 & 2 \\
                      \end{array} \right)
\]
and $\tilde{b}_i=h^2f_i$.

In our case we will assume  that $f(x) = (3x+x^2)e^x$, and keep the same interval and boundary 
conditions. Then the above differential equation
has an analytic solution given by $u(x) = x(1-x)e^x$ (convince yourself that this is correct by inserting the
solution in the Poisson equation).  We will compare
our numerical solution with this analytic result in the next exercise. 

\item
We can rewrite our matrix ${\bf A}$ in terms of one-dimensional vectors $a,b,c$  
of length $1:n$. 
Our linear equation reads
\[
    {\bf A} = \left(\begin{array}{cccccc}
                           b_1& c_1 & 0 &\dots   & \dots &\dots \\
                           a_2 & b_2 & c_2 &\dots &\dots &\dots \\
                           & a_3 & b_3 & c_3 & \dots & \dots \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &a_{n-2}  &b_{n-1}& c_{n-1} \\
                           &    &  &   &a_n & b_n \\
                      \end{array} \right)\left(\begin{array}{c}
                           v_1\\
                           v_2\\
                           \dots \\
                          \dots  \\
                          \dots \\
                           v_n\\
                      \end{array} \right)
  =\left(\begin{array}{c}
                           \tilde{b}_1\\
                           \tilde{b}_2\\
                           \dots \\
                           \dots \\
                          \dots \\
                           \tilde{b}_n\\
                      \end{array} \right).
\]
A tridiagonal matrix is a special form of banded matrix where all the elements are zero except for 
those on and immediately above and below the leading diagonal.
The above tridiagonal system   can be written as
\[
  a_iv_{i-1}+b_iv_i+c_iv_{i+1} = \tilde{b}_i,
\]
for $i=1,2,\dots,n$. 
The algorithm for solving this set of equations is rather simple and requires two steps only, a decomposition 
and forward substitution and finally a backward substitution. 


Your first task is to set up the algorithm for solving this set of linear equations.
Find also the number of operations needed to solve the above equations. Show that they behave like $O(n)$ with 
$n$ the dimensionality of the problem. Compare this with standard Gaussian elimination.  

Then you should code the above algorithm and solve the problem for matrices of the size
$10\times 10$, $100\times 100$ and $1000\times 1000$.  That means that you choose $n=10$, $n=100$ and 
$n=1000$ grid points. 

Compare your results (make plots) with the analytic results for the different number of grid points  in the 
interval $x\in(0,1)$.  The different number of grid points corresponds to different step lengths $h$.


Compute also the maximal relative error  in the data set $i=1,\dots, n$,by setting up 
\[
   \epsilon_i=log_{10}\left(\left|\frac{v_i-u_i}
                 {u_i}\right|\right),
\]
as function of $log_{10}(h)$ for the function values $u_i$ and $v_i$.
For each step length extract the max value of the relative error.  
Try to increase $n$ to $n=10000$ and $n=10^5$.  Comment your results. 

\item
Compare your results with those from the LU decomposition codes for the matrix of size
$1000\times 1000$.
Use for example the unix function {\em time} when you run your codes 
and compare the time usage between LU decomposition and  your
tridiagonal solver.   Can you run the standard LU decomposition
for a matrix of the size $10^5\times 10^5$?
Comment your results.
\end{enumerate}

\subsection{Solution}
The program listed below encodes a possible solution to part b) of the above project.
Note that we have employed Blitz++ as library and that the range of the various vectors are now shifted 
from their default ranges $(0:n-1)$ to $(1:n)$ and that we access vector elements as $a(i)$ instead of the 
standard C++ declaration $a[i]$.

The program reads from screen the name of the ouput file and the dimension of 
the problem, which in our case corresponds to the number of mesh points as well, in addition to
the two endpoints.  The function $f(x) = (3x+x^2)\exp{(x)}$ is included explicitely in the code.
An obvious change is to define a separate function, allowing thereby for a generalization
to other function $f(x)$. 
\begin{lstlisting}
/*   
    Program to solve the one-dimensional Poisson equation  
    -u''(x) = f(x)  rewritten as a set of linear equations
    A u = f   where A is an n x n matrix, and u and f are 1 x n vectors
    In this problem f(x) = (3x+x*x)exp(x)  with solution u(x) = x(1-x)exp(x)
    The program reads  from screen the name of the output file.
    Blitz++ is used here, with arrays starting from 1 to n
*/
#include <iomanip>
#include <fstream>
#include <blitz/array.h>
#include <iostream>
using namespace std;
using namespace blitz;

ofstream ofile;
//  Main program only, no other functions
int main(int argc, char* argv[])
{
  char *outfilename;
  int i, j, n;
  double h, btemp;
  // Read in output file, abort if there are too few command-line arguments
  if( argc <= 1 ){
    cout << "Bad Usage: " << argv[0] <<
      " read also output file on same line" << endl;
    exit(1);
  }
  else{
    outfilename=argv[1];
  }
  ofile.open(outfilename);
  cout << "Read in number of mesh points" << endl;
  cin >> n; 
  h =  1.0/( (double) n+1);
  //  Use Blitz to allocate arrays
  //  Use range to change default arrays from 0:n-1 to 1:n
  Range r(1,n); 
  Array<double,1> a(r), b(r), c(r), y(r), f(r), temp(r);  
  //  set up the matrix defined by three arrays, diagonal, upper and lower diagonal band 
  b = 2.0;  a = -1.0  ; c = -1.0;   
  // Then define the value of the right hand side f (multiplied by h*h)
  for(i=1; i <= n; i++){
    // Explicit expression for f, could code as separate function
    f(i) = h*h*(i*h*3.0+(i*h)*(i*h))*exp(i*h);
  }
  // solve the tridiagonal system, first forward substitution
  btemp = b(1);  
  for(i = 2; i <= n; i++) {
    temp(i) = c(i-1) / btemp;
    btemp = b(i) - a(i) * temp(i);
    y(i) = (f(i) - a(i) * y(i-1)) / btemp;
  }
  // then backward substitution, the solution is in y()
  for(i = n-1; i >= 1; i--) {
    y(i) -= temp(i+1) * y(i+1);
  }
  // write results to the output file
  for(i = 1; i <= n; i++){
    ofile << setiosflags(ios::showpoint | ios::uppercase);
    ofile << setw(15) << setprecision(8) << i*h;
    ofile << setw(15) << setprecision(8) << y(i);
    ofile << setw(15) << setprecision(8) << i*h*(1.0-i*h)*exp(i*h) <<endl;
  }
  ofile.close(); 
}
\end{lstlisting}
The program writes also the exact solution to file.  
\begin{figure}
\begin{center}
\input{figures/solutionchap4_10}
\end{center}
\caption{Numerical solution obtained with $n=10$  compared with the analytical solution.\label{fig:project1fig1}} 
\end{figure}
In Fig.~\ref{fig:project1fig1} we show the 
results obtained with $n=10$. Even with so few points, the numerical solution is very close to the analytic 
answer. With $n=100$ it is almost impossible to distinguish the numerical solution from the analytical one,
as shown in Fig.~\ref{fig:project1fig2}.
\begin{figure}
\begin{center}
\input{figures/solutionchap4_100}
\end{center}
\caption{Numerical solution obtained with $n=10$  compared with the analytical solution.\label{fig:project1fig2}} 
\end{figure} 
It is therefore instructive to study the relative error, which we display in Table \ref{tab:log10relative}
as function of the step length $h=1/(n+1)$.


\begin{table}[hbtp]
\begin{center}
\caption{$log_{10}$ values for the relative error and the step length $h$ computed at $x=0.5$.\label{tab:log10relative}}
\begin{tabular}{rll}\hline
$n$&$log_ {10}(h)$&$\epsilon_i=log_{10}\left(\left|(v_i-u_i)/u_i\right|\right)$\\\hline
10 &-1.04 & -2.29  \\
100 & -2.00 & -4.19    \\
1000&-3.00  &-6.18     \\
$10^4$& -4.00 &-8.18    \\
$10^5$& -5.00 &-9.19    \\
$10^6$& -6.00 &-6.08    \\
\hline
\end{tabular} 
\end{center}   
\end{table}     
The mathematical truncation we made when computing the second derivative goes like $O(h^2)$.
Our results for $n$  from $n=10$ to somewhere between $n=10^4$ and $n=10^5$
result in a slope which is almost exactly equal $2$,in good agreement with the mathematical truncation made.
Beyond $n=10^5$ the relative error becomes bigger, telling us that there is no point in increasing $n$.
For most practical application a relative error between $10^{-6}$ and $10^{-8}$ is more than
sufficient, meaning that $n=10^4$ may be an acceptable number of mesh points. Beyond $n=10^5$, numerical
round off errors take over, as discussed in the previous chapter  as well. 
\end{prob}


\begin{prob}
Write your own code for performing the cubic spline interpolation using either Blitz++ or Armadillo. Alternatively you can use the vector-matrix class included in this text. 
\end{prob}

\begin{prob}
Write your own code for the LU decomposition using the same libraries as in the previous exercise.  Find also the number of floating point operations.
\end{prob}

\begin{prob}
Solve exercise 6.3 by writing a code which implements both the iterative Jacobi method and
the Gauss-Seidel method. Study carefully the number of iterations needed to achieve the exact result.
\end{prob}

\begin{prob}
Extend thereafter your code for the iterative Jacobi method to a parallel version and compare
with the results from the previous exercise.
\end{prob}


\begin{prob}
Write your own code for the Conjugate gradient method.
\end{prob}

\begin{prob}
Write your own code for matrix-matrix multiplications using Strassen's algorithm  discussed in subsection \ref{subsubsec:strassenalgo} and compare the speed of your program with the matrix-matrix multiplication provided by the Armadillo library.
\end{prob}



\bibliographystyle{plain}
\bibliography{IntroductoryBook}








